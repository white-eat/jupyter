{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:13:48.870947Z",
     "start_time": "2023-03-16T06:13:48.850991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:13:49.527838Z",
     "start_time": "2023-03-16T06:13:49.516377Z"
    }
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "input_window = 100\n",
    "output_window = 5\n",
    "batch_size = 32 # batch size\n",
    "\n",
    "# This concept is also called teacher forceing. \n",
    "# The flag decides if the loss will be calculted over all \n",
    "# or just the predicted values.\n",
    "# calculate_loss_over_all_values = False\n",
    "calculate_loss_over_all_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:13:50.603940Z",
     "start_time": "2023-03-16T06:13:50.589948Z"
    }
   },
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=20000):\n",
    "        super(PositionalEncoding, self).__init__()  \n",
    "        # max_len参数指定模型将能够处理的最大序列长度，而d_model参数指定输入嵌入的维数\n",
    "        # pe将为输入序列中的每个位置（最多max_len）提供一行，并为每个嵌入维度提供d_model列。\n",
    "        # 计算位置编码的值\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        i = torch.arange(d_model).unsqueeze(0)\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model).float())\n",
    "        angle_rads = pos * angle_rates\n",
    "        # 使用正弦和余弦函数来产生位置编码\n",
    "        pe = torch.zeros((max_len, d_model))\n",
    "        pe[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:13:51.196407Z",
     "start_time": "2023-03-16T06:13:51.169479Z"
    }
   },
   "outputs": [],
   "source": [
    "# 模型架构\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=7,num_layers=3,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=7, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,feature_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            # print('a',src.size())\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        src.size()\n",
    "        # print('j',src.size(),self.src_mask.size())\n",
    "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:13:51.948020Z",
     "start_time": "2023-03-16T06:13:51.927076Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分数据窗口\n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        # 数据和标签\n",
    "        # 对数据进行最后output_window窗口的预测，因此用0代替\n",
    "        train_seq = np.append(input_data[i:i+tw,:][:-output_window,:] , np.zeros((output_window,7)),axis=0)\n",
    "        train_label = input_data[i:i+tw,:]\n",
    "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    # 转化成张量\n",
    "    return torch.FloatTensor(inout_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:13:52.730947Z",
     "start_time": "2023-03-16T06:13:52.711677Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据并处理\n",
    "def get_data():\n",
    "    \n",
    "    # 导入原数据\n",
    "    data = pd.read_csv('./ETTDataset/ETTh1.csv')\n",
    "    c_all=pd.Series(data.columns.drop('date'))\n",
    "    # 归一化\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    data_c_all = data.loc[:, c_all]\n",
    "    series = data_c_all.to_numpy()\n",
    "    amplitude = scaler.fit_transform(series)\n",
    "    # amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    # 切分训练集和测试集\n",
    "    split_train = int(amplitude.shape[0] * 0.7)\n",
    "    train = amplitude[:split_train]\n",
    "    test_data = amplitude[split_train:]\n",
    "    split_test = int(test_data.shape[0] * 0.5)\n",
    "    valid = test_data[:split_test]\n",
    "    test = test_data[split_test:]\n",
    "    \n",
    "    # 窗口为input_window，将数据划分成(带标签的)训练和测试数据，并转化为张量\n",
    "    # 转化训练数据\n",
    "    train_sequence = create_inout_sequences(train,input_window)\n",
    "    # 最后 output_window 为预测窗口的大小\n",
    "    train_sequence = train_sequence[:-output_window]\n",
    "    \n",
    "    # 转化测试数据\n",
    "    valid_sequence = create_inout_sequences(valid,input_window)\n",
    "    valid_sequence = valid_sequence[:-output_window]\n",
    "    \n",
    "    # 转化测试数据\n",
    "    test_sequence = create_inout_sequences(test,input_window)\n",
    "    test_sequence = test_sequence[:-output_window]\n",
    "\n",
    "    return train_sequence.to(device), valid_sequence.to(device), test_sequence.to(device),scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:13:53.791247Z",
     "start_time": "2023-03-16T06:13:53.776409Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据转化：输入标签\n",
    "def get_batch(source, i,batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]   \n",
    "    # 数据切分成 输入和标签\n",
    "    # 数据维度转化为[input_window,batch_size,1]\n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)).squeeze() # 1 is feature size\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1)).squeeze()\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:14:03.227234Z",
     "start_time": "2023-03-16T06:13:54.638963Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12089, 2, 100, 7]) torch.Size([2508, 2, 100, 7]) torch.Size([2508, 2, 100, 7])\n",
      "torch.Size([100, 32, 7]) torch.Size([100, 32, 7])\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data, scaler = get_data()\n",
    "print(train_data.size(), valid_data.size(), test_data.size())\n",
    "# print(train_data.size(), val_data.size())\n",
    "tr,te = get_batch(train_data, 0,batch_size)\n",
    "print(tr.shape,te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T04:45:14.754615Z",
     "start_time": "2023-03-16T04:45:14.732676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32, 7])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T04:45:15.949108Z",
     "start_time": "2023-03-16T04:45:15.935145Z"
    }
   },
   "outputs": [],
   "source": [
    "tr1,te1 = get_batch(train_data, 10,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T04:45:18.327402Z",
     "start_time": "2023-03-16T04:45:18.286956Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9366e-01, -1.8023e-02,  2.7639e-01,  ..., -1.4454e-01,\n",
       "           2.9635e-01, -3.9312e-02],\n",
       "         [ 2.2831e-01, -1.8023e-02,  2.8643e-01,  ..., -4.3981e-02,\n",
       "           2.9635e-01, -3.3721e-02],\n",
       "         [ 1.8209e-01, -1.8023e-02,  2.6800e-01,  ..., -1.6973e-01,\n",
       "           3.1039e-01, -7.0218e-02],\n",
       "         ...,\n",
       "         [ 4.3064e-01,  3.3329e-01,  5.1090e-01,  ..., -1.8233e-01,\n",
       "           4.0684e-01,  4.2147e-02],\n",
       "         [ 4.0462e-01,  2.8837e-01,  5.1425e-01,  ..., -1.5713e-01,\n",
       "           4.2087e-01, -1.1240e-02],\n",
       "         [ 4.0751e-01,  2.8837e-01,  5.0755e-01,  ..., -8.7962e-02,\n",
       "           4.0684e-01,  1.7979e-01]],\n",
       "\n",
       "        [[ 2.2831e-01, -1.8023e-02,  2.8643e-01,  ..., -4.3981e-02,\n",
       "           2.9635e-01, -3.3721e-02],\n",
       "         [ 1.8209e-01, -1.8023e-02,  2.6800e-01,  ..., -1.6973e-01,\n",
       "           3.1039e-01, -7.0218e-02],\n",
       "         [ 1.8209e-01, -3.2067e-08,  2.6635e-01,  ..., -1.1315e-01,\n",
       "           3.5160e-01, -9.5494e-02],\n",
       "         ...,\n",
       "         [ 4.0462e-01,  2.8837e-01,  5.1425e-01,  ..., -1.5713e-01,\n",
       "           4.2087e-01, -1.1240e-02],\n",
       "         [ 4.0751e-01,  2.8837e-01,  5.0755e-01,  ..., -8.7962e-02,\n",
       "           4.0684e-01,  1.7979e-01],\n",
       "         [ 4.3931e-01,  2.7034e-01,  5.0920e-01,  ..., -5.6577e-02,\n",
       "           4.3446e-01,  1.9664e-01]],\n",
       "\n",
       "        [[ 1.8209e-01, -1.8023e-02,  2.6800e-01,  ..., -1.6973e-01,\n",
       "           3.1039e-01, -7.0218e-02],\n",
       "         [ 1.8209e-01, -3.2067e-08,  2.6635e-01,  ..., -1.1315e-01,\n",
       "           3.5160e-01, -9.5494e-02],\n",
       "         [ 1.9940e-01,  3.6046e-02,  2.9817e-01,  ..., -2.2631e-01,\n",
       "           2.6919e-01, -5.6202e-02],\n",
       "         ...,\n",
       "         [ 4.0751e-01,  2.8837e-01,  5.0755e-01,  ..., -8.7962e-02,\n",
       "           4.0684e-01,  1.7979e-01],\n",
       "         [ 4.3931e-01,  2.7034e-01,  5.0920e-01,  ..., -5.6577e-02,\n",
       "           4.3446e-01,  1.9664e-01],\n",
       "         [ 4.1618e-01,  2.2529e-01,  4.9582e-01,  ..., -1.1315e-01,\n",
       "           4.0684e-01,  2.7526e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:42:40.388831Z",
     "start_time": "2023-03-16T06:42:40.352959Z"
    }
   },
   "outputs": [],
   "source": [
    "tr2,te2 = get_batch(train_data, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:43:17.853567Z",
     "start_time": "2023-03-16T06:43:17.834588Z"
    }
   },
   "outputs": [],
   "source": [
    "tr2.shape\n",
    "tr2 = tr2.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:43:25.367629Z",
     "start_time": "2023-03-16T06:43:25.357954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 7])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:14:30.092523Z",
     "start_time": "2023-03-16T06:14:30.078558Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train(model, optimizer, criterion, scheduler, epoch, train_data):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        # 数据切分成 输入数据和标签\n",
    "        data, targets = get_batch(train_data, i,batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        # calculate_loss_over_all_values 为True时，损失将在所有输出值和目标值上计算\n",
    "        # 为False时，损失仅计算 output 和 targets 的最后 output_window 个值\n",
    "        if calculate_loss_over_all_values:\n",
    "            loss = criterion(output, targets)\n",
    "        else:\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "    \n",
    "        loss.backward()\n",
    "        # 执行梯度裁剪的函数,parameters参数是表示神经网络参数梯度的张量列表\n",
    "        # max_norm参数是梯度的最大范数，即当参数的范数大于max_norm时，就会对梯度进行削减\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 计算累积的损失值\n",
    "        total_loss += loss.item()\n",
    "        # 计算打印日志的间隔，设置为训练集大小的5分之一\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        # batch 是 log_interval 的倍数且不是第 0 个 batch时，开始打印日志\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # 计算当前平均损失\n",
    "            cur_loss = total_loss / log_interval\n",
    "            # 计算从训练开始到现在的时间\n",
    "            elapsed = time.time() - start_time\n",
    "            # 打印日志，其中包括当前 epoch，batch，学习率，时间，损失值和困惑度\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            # 重置 total_loss，以便下一个 log_interval 计算新的平均损失值\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:14:31.773710Z",
     "start_time": "2023-03-16T06:14:31.726838Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source,epoch,scaler):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            data = data.unsqueeze(1)\n",
    "            target = target.unsqueeze(1)\n",
    "\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)   \n",
    "\n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "            \n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1,:].squeeze(1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1,:].squeeze(1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "    \n",
    "    print(test_result.size(),truth.size())\n",
    "    test_result=scaler.inverse_transform(test_result.reshape(-1, 1)).reshape(-1)\n",
    "    truth=scaler.inverse_transform(truth.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    pyplot.plot(test_result,color=\"red\")\n",
    "    pyplot.plot(truth[:500],color=\"blue\")\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.xlabel(\"Periods\")\n",
    "    pyplot.ylabel(\"Y\")\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    pyplot.close()\n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "# 预测将来的时间步，steps 表示预测的步长\n",
    "def predict_future(eval_model, data_source,steps,epoch,scaler):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    # 这里的 data 为真值\n",
    "    _ , data = get_batch(data_source, 0,1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps,1):\n",
    "            input = torch.clone(data[-input_window:])\n",
    "            input[-output_window:] = 0     \n",
    "            output = eval_model(data[-input_window:])                        \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "    data = data.cpu().view(-1)\n",
    "    \n",
    "    data=scaler.inverse_transform(data.reshape(-1, 1)).reshape(-1)\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png'%epoch)\n",
    "    pyplot.close()\n",
    "        \n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    # 设置评估输入的batch大小 eval_batch_size\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data) \n",
    "\n",
    "            if calculate_loss_over_all_values:\n",
    "                # len(data[0]) eval_batch_size的大小乘以损失\n",
    "                total_loss += len(data[0])* criterion(output, targets).item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-output_window:], targets[-output_window:]).item()   \n",
    "\n",
    "    # 对所有验证数据输出 损失\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测将来的时间步，steps 表示预测的步长\n",
    "def predict_future(eval_model, data_source, steps, scaler):\n",
    "    eval_model.eval() \n",
    "\n",
    "    # 这里的 target 为真值, 取第一个数据[1, 2, 100, 7]进行预测将来。\n",
    "    data, target = get_batch(data_source, 0, 1)\n",
    "    data = data.unsqueeze(1)\n",
    "    target = target.unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps, 1):\n",
    "            input = torch.clone(data[-input_window:]) \n",
    "            output = eval_model(input)\n",
    "            # 只将最后一行预测 进行加入\n",
    "            data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "    data = data.cpu().view(-1)\n",
    "    \n",
    "    data=scaler.inverse_transform(data.reshape(-1, 1)).reshape(-1)\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png'%steps)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:24:55.942134Z",
     "start_time": "2023-03-16T07:24:55.925309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 100, 7])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:58:57.493726Z",
     "start_time": "2023-03-16T06:58:57.466736Z"
    }
   },
   "outputs": [],
   "source": [
    "data, targets = get_batch(test_data, 0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:18:36.377088Z",
     "start_time": "2023-03-16T07:18:36.356915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2508, 2, 100, 7])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:59:06.881085Z",
     "start_time": "2023-03-16T06:59:06.867083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1000, 7])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:08:50.897569Z",
     "start_time": "2023-03-16T07:08:50.886746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 7])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:31:42.973944Z",
     "start_time": "2023-03-16T07:31:42.951973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 7])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ , data = get_batch(test_data, 10,1)\n",
    "data = data.unsqueeze(1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:31:44.203817Z",
     "start_time": "2023-03-16T07:31:44.142267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.8324e-01, -1.7108e-01, -2.7304e-01, -7.0243e-02, -1.5094e-01,\n",
       "           1.1750e-01, -6.1519e-01]],\n",
       "\n",
       "        [[-5.5780e-01,  1.1715e-01, -5.7284e-01,  1.8442e-01, -6.2978e-02,\n",
       "           1.9991e-01, -5.5901e-01]],\n",
       "\n",
       "        [[-6.6188e-01,  1.3517e-01, -7.2528e-01,  1.6877e-01, -1.2595e-02,\n",
       "           1.4467e-01, -5.7865e-01]],\n",
       "\n",
       "        [[-7.8326e-01,  9.0114e-02, -8.2243e-01,  4.9338e-02, -2.2011e-01,\n",
       "           6.2259e-02, -5.7586e-01]],\n",
       "\n",
       "        [[-7.9771e-01,  1.9825e-01, -7.7888e-01,  1.6351e-01, -2.3890e-01,\n",
       "           4.8223e-02, -5.4775e-01]],\n",
       "\n",
       "        [[-7.3989e-01,  1.2616e-01, -7.5880e-01,  1.5313e-01, -2.7049e-01,\n",
       "           8.9880e-02, -4.6349e-01]],\n",
       "\n",
       "        [[-5.1159e-01,  6.3080e-02, -5.9966e-01,  6.4834e-02,  2.7049e-01,\n",
       "           1.7229e-01, -4.9719e-01]],\n",
       "\n",
       "        [[-2.1679e-01, -1.0814e-01, -2.7974e-01, -9.1002e-02,  3.1447e-01,\n",
       "           2.2753e-01, -4.6349e-01]],\n",
       "\n",
       "        [[ 9.5361e-02, -2.0713e-01,  1.5916e-01, -1.2726e-01, -8.1768e-02,\n",
       "           1.3108e-01, -4.4944e-01]],\n",
       "\n",
       "        [[ 3.3238e-01, -1.2616e-01,  4.1875e-01, -5.4601e-02, -8.1768e-02,\n",
       "          -2.0602e-02, -4.8314e-01]],\n",
       "\n",
       "        [[ 3.7571e-01, -1.6207e-01,  4.5226e-01, -7.5360e-02, -5.0382e-02,\n",
       "          -4.8223e-02, -5.1684e-01]],\n",
       "\n",
       "        [[ 3.7282e-01, -1.5306e-01,  4.4891e-01, -7.5360e-02, -5.0382e-02,\n",
       "          -3.4639e-02, -5.5617e-01]],\n",
       "\n",
       "        [[ 3.7860e-01, -1.4405e-01,  4.5396e-01, -1.2214e-01, -5.0382e-02,\n",
       "           4.8223e-02, -5.6460e-01]],\n",
       "\n",
       "        [[ 4.6244e-01,  1.8023e-02,  5.6450e-01,  1.3237e-01, -1.3215e-01,\n",
       "           3.4639e-02, -5.7302e-01]],\n",
       "\n",
       "        [[ 6.2429e-01,  2.8837e-01,  6.7004e-01,  2.2067e-01,  3.5226e-01,\n",
       "           6.2780e-01, -5.8145e-01]],\n",
       "\n",
       "        [[ 6.7051e-01,  5.0451e-01,  7.3033e-01,  4.0779e-01,  1.9492e-01,\n",
       "           1.3108e-01, -5.7023e-01]],\n",
       "\n",
       "        [[ 7.1387e-01,  4.0538e-01,  7.6049e-01,  3.4537e-01,  2.0752e-01,\n",
       "           1.3108e-01, -5.6460e-01]],\n",
       "\n",
       "        [[ 5.9249e-01,  1.8023e-01,  6.3652e-01,  2.1555e-01,  2.1392e-01,\n",
       "           2.1395e-01, -5.6180e-01]],\n",
       "\n",
       "        [[ 6.0117e-01,  1.9825e-01,  6.4322e-01,  1.9480e-01,  1.5094e-01,\n",
       "           1.1750e-01, -5.6180e-01]],\n",
       "\n",
       "        [[ 5.7804e-01,  1.3517e-01,  6.0805e-01,  1.9991e-01,  2.5790e-01,\n",
       "           1.4467e-01, -5.8428e-01]],\n",
       "\n",
       "        [[ 5.7804e-01,  2.1627e-01,  6.0805e-01,  1.2726e-01,  2.6409e-01,\n",
       "           1.9991e-01, -5.8428e-01]],\n",
       "\n",
       "        [[ 6.5316e-01,  2.1627e-01,  6.6499e-01,  1.4275e-01,  3.3326e-01,\n",
       "           2.5515e-01, -5.7302e-01]],\n",
       "\n",
       "        [[ 6.7629e-01,  2.7935e-01,  6.6834e-01,  2.3631e-01,  5.4718e-01,\n",
       "           3.5160e-01, -6.1235e-01]],\n",
       "\n",
       "        [[ 1.2138e-01,  1.4418e-01,  1.7589e-01,  2.4158e-01, -3.7787e-02,\n",
       "           8.9880e-02, -6.0393e-01]],\n",
       "\n",
       "        [[-2.7456e-01,  8.1103e-02, -2.3284e-01,  1.5313e-01, -1.1315e-01,\n",
       "           1.3108e-01, -6.0956e-01]],\n",
       "\n",
       "        [[-5.1159e-01,  2.2529e-01, -4.7739e-01,  3.1423e-01, -1.5094e-01,\n",
       "           1.4467e-01, -5.5617e-01]],\n",
       "\n",
       "        [[-6.4742e-01,  1.1715e-01, -6.5491e-01, -4.4222e-02, -8.7962e-02,\n",
       "           6.2259e-02, -5.7023e-01]],\n",
       "\n",
       "        [[-7.8904e-01,  5.4069e-02, -7.8727e-01, -9.6119e-02, -2.7049e-01,\n",
       "          -2.0602e-02, -5.2810e-01]],\n",
       "\n",
       "        [[-6.5031e-01, -3.2067e-08, -6.2144e-01,  8.5593e-02, -2.9548e-01,\n",
       "           7.0183e-03, -5.0562e-01]],\n",
       "\n",
       "        [[-6.3008e-01,  1.8924e-01, -7.0685e-01,  9.6119e-02,  2.5150e-01,\n",
       "           2.8277e-01, -4.2416e-01]],\n",
       "\n",
       "        [[-3.8438e-01,  4.5057e-02, -4.5895e-01, -2.7045e-03,  2.8928e-01,\n",
       "           2.6919e-01, -4.4101e-01]],\n",
       "\n",
       "        [[ 6.6451e-02, -1.4405e-01,  4.1882e-02, -1.1688e-01,  3.0188e-01,\n",
       "           2.5515e-01, -4.4944e-01]],\n",
       "\n",
       "        [[ 1.9077e-01, -2.0713e-01,  2.5631e-01, -1.2214e-01, -7.5367e-02,\n",
       "          -7.0183e-03, -4.4944e-01]],\n",
       "\n",
       "        [[ 3.0926e-01, -2.1614e-01,  3.9197e-01, -1.5328e-01, -1.0056e-01,\n",
       "           7.0183e-03, -4.8034e-01]],\n",
       "\n",
       "        [[ 3.6414e-01, -2.1614e-01,  4.4052e-01, -1.3778e-01, -5.0382e-02,\n",
       "          -7.0183e-03, -4.9719e-01]],\n",
       "\n",
       "        [[ 3.3238e-01, -2.5219e-01,  4.1540e-01, -1.6892e-01, -1.0056e-01,\n",
       "          -3.4639e-02, -5.0283e-01]],\n",
       "\n",
       "        [[ 3.8438e-01, -9.0114e-02,  4.7908e-01, -1.8200e-02, -1.2575e-01,\n",
       "           3.4639e-02, -5.2247e-01]],\n",
       "\n",
       "        [[ 4.2775e-01,  1.7122e-01,  5.4941e-01,  1.5313e-01, -2.2631e-01,\n",
       "           7.0183e-03, -5.3653e-01]],\n",
       "\n",
       "        [[ 5.9249e-01,  3.3329e-01,  7.1025e-01,  3.7651e-01, -1.6354e-01,\n",
       "           4.8223e-02, -5.3090e-01]],\n",
       "\n",
       "        [[ 5.7515e-01,  4.5044e-01,  5.9966e-01,  3.5575e-01,  3.3326e-01,\n",
       "           2.2753e-01, -5.1684e-01]],\n",
       "\n",
       "        [[ 5.4913e-01,  4.0538e-01,  5.7454e-01,  4.0779e-01,  2.2651e-01,\n",
       "           2.6919e-01, -5.1968e-01]],\n",
       "\n",
       "        [[ 5.6936e-01,  3.6933e-01,  6.1474e-01,  3.7139e-01,  1.2575e-01,\n",
       "           1.0346e-01, -5.3373e-01]],\n",
       "\n",
       "        [[ 5.3756e-01,  2.9738e-01,  5.8123e-01,  3.4025e-01,  1.3215e-01,\n",
       "           8.9880e-02, -5.3090e-01]],\n",
       "\n",
       "        [[ 5.5491e-01,  4.4143e-01,  5.8123e-01,  4.0253e-01,  1.4474e-01,\n",
       "           1.3108e-01, -5.1968e-01]],\n",
       "\n",
       "        [[ 5.2311e-01,  2.9738e-01,  5.5276e-01,  2.7271e-01,  2.3271e-01,\n",
       "           1.9991e-01, -5.1125e-01]],\n",
       "\n",
       "        [[ 5.2311e-01,  7.2091e-02,  5.6280e-01,  7.5214e-02,  3.2067e-01,\n",
       "           2.9635e-01, -5.3090e-01]],\n",
       "\n",
       "        [[ 6.0117e-01,  2.4331e-01,  5.5446e-01,  1.2726e-01,  4.5922e-01,\n",
       "           1.9991e-01, -5.3932e-01]],\n",
       "\n",
       "        [[ 1.0982e-01,  6.3080e-02,  1.6750e-01,  1.4275e-01, -1.1955e-01,\n",
       "           4.8223e-02, -5.1684e-01]],\n",
       "\n",
       "        [[-3.1215e-01, -1.1715e-01, -2.5127e-01, -3.8959e-02, -1.5713e-01,\n",
       "           1.0346e-01, -5.0562e-01]],\n",
       "\n",
       "        [[-5.3756e-01,  5.4069e-02, -5.2594e-01,  1.2938e-02, -1.8873e-01,\n",
       "           1.5870e-01, -4.8314e-01]],\n",
       "\n",
       "        [[-6.4164e-01,  4.5057e-02, -6.3652e-01,  4.9338e-02, -1.3215e-01,\n",
       "           3.4639e-02, -4.8877e-01]],\n",
       "\n",
       "        [[-7.9193e-01,  5.4069e-02, -7.7553e-01,  5.4455e-02, -3.3326e-01,\n",
       "          -7.5843e-02, -4.7755e-01]],\n",
       "\n",
       "        [[-6.8207e-01,  3.8736e-01, -7.6714e-01,  1.9991e-01,  1.9492e-01,\n",
       "           5.5852e-01, -4.6349e-01]],\n",
       "\n",
       "        [[-6.2140e-01,  2.7034e-02, -7.2194e-01, -2.7045e-03,  2.2651e-01,\n",
       "           2.5515e-01, -4.1574e-01]],\n",
       "\n",
       "        [[-4.4509e-01, -5.4069e-02, -5.4772e-01, -9.1002e-02,  2.4530e-01,\n",
       "           2.9635e-01, -4.3259e-01]],\n",
       "\n",
       "        [[-2.5722e-01, -2.0713e-01, -2.8473e-01, -1.8968e-01,  2.7049e-01,\n",
       "           2.8277e-01, -4.1853e-01]],\n",
       "\n",
       "        [[ 2.3128e-02, -3.0625e-01,  1.3738e-01, -3.0400e-01, -1.2575e-01,\n",
       "           3.4639e-02, -3.9046e-01]],\n",
       "\n",
       "        [[ 2.9769e-01, -3.8736e-01,  3.9362e-01, -3.1438e-01, -1.6354e-01,\n",
       "          -6.1807e-02, -3.9889e-01]],\n",
       "\n",
       "        [[ 3.5258e-01, -3.6032e-01,  4.2549e-01, -3.2476e-01, -6.1945e-03,\n",
       "           6.2259e-02, -4.4101e-01]],\n",
       "\n",
       "        [[ 3.4391e-01, -3.6032e-01,  4.1375e-01, -3.1949e-01, -3.7787e-02,\n",
       "           1.1750e-01, -4.4381e-01]],\n",
       "\n",
       "        [[ 2.9769e-01, -3.3329e-01,  4.1040e-01, -2.5722e-01, -1.6973e-01,\n",
       "           6.2259e-02, -4.6070e-01]],\n",
       "\n",
       "        [[ 4.0173e-01, -7.2091e-02,  5.3602e-01,  3.3696e-02, -2.5150e-01,\n",
       "          -3.4639e-02, -4.6629e-01]],\n",
       "\n",
       "        [[ 6.4738e-01,  2.1627e-01,  6.7838e-01,  2.3631e-01,  2.2011e-01,\n",
       "           1.7229e-01, -4.6912e-01]],\n",
       "\n",
       "        [[ 6.5894e-01,  3.6933e-01,  6.9681e-01,  2.7783e-01,  1.6353e-01,\n",
       "           8.9880e-02, -4.7755e-01]],\n",
       "\n",
       "        [[ 6.6183e-01,  4.1439e-01,  7.0520e-01,  3.7651e-01,  1.8233e-01,\n",
       "           1.1750e-01, -4.8877e-01]],\n",
       "\n",
       "        [[ 5.5780e-01,  2.6133e-01,  6.1809e-01,  2.6745e-01,  1.0696e-01,\n",
       "           4.8223e-02, -4.8314e-01]],\n",
       "\n",
       "        [[ 5.9249e-01,  3.2428e-01,  6.6834e-01,  3.7139e-01,  1.1955e-01,\n",
       "           8.9880e-02, -4.8597e-01]],\n",
       "\n",
       "        [[ 5.6647e-01,  3.6032e-01,  6.3318e-01,  2.8821e-01,  1.4474e-01,\n",
       "           1.4467e-01, -4.8877e-01]],\n",
       "\n",
       "        [[ 6.6183e-01,  4.1439e-01,  7.0685e-01,  3.9215e-01,  2.3271e-01,\n",
       "           1.3108e-01, -4.8877e-01]],\n",
       "\n",
       "        [[ 6.5894e-01,  3.0639e-01,  6.4157e-01,  2.6233e-01,  3.3326e-01,\n",
       "           3.1039e-01, -4.9440e-01]],\n",
       "\n",
       "        [[ 5.6647e-01,  1.3517e-01,  5.4442e-01,  5.9718e-02,  4.1503e-01,\n",
       "           2.2753e-01, -5.0842e-01]],\n",
       "\n",
       "        [[ 5.1996e-02,  2.7034e-02,  1.4573e-01,  8.0477e-02, -1.2575e-01,\n",
       "          -7.0183e-03, -5.1125e-01]],\n",
       "\n",
       "        [[-3.2371e-01, -1.2616e-01, -2.8139e-01, -1.3084e-02, -2.0132e-01,\n",
       "           6.2259e-02, -4.6629e-01]],\n",
       "\n",
       "        [[-4.9135e-01,  8.1103e-02, -4.9582e-01,  1.4275e-01, -1.5713e-01,\n",
       "           6.2259e-02, -3.9889e-01]],\n",
       "\n",
       "        [[-6.0695e-01,  1.2616e-01, -6.0635e-01,  1.0650e-01, -1.3215e-01,\n",
       "           1.0346e-01, -4.0452e-01]],\n",
       "\n",
       "        [[-7.7458e-01, -5.4069e-02, -7.7723e-01,  1.8200e-02, -2.9548e-01,\n",
       "          -7.0183e-03, -4.0168e-01]],\n",
       "\n",
       "        [[-7.4278e-01,  6.3080e-02, -7.1020e-01,  9.0856e-02, -3.1447e-01,\n",
       "          -7.0183e-03, -3.7361e-01]],\n",
       "\n",
       "        [[-6.2140e-01,  4.5057e-02, -5.6784e-01,  1.3237e-01, -3.1447e-01,\n",
       "           7.5843e-02, -3.3148e-01]],\n",
       "\n",
       "        [[-4.3353e-01, -7.2091e-02, -3.9028e-01,  7.0097e-02, -3.4586e-01,\n",
       "           6.2259e-02, -3.6798e-01]],\n",
       "\n",
       "        [[-1.7920e-01, -1.8911e-01, -9.2130e-02, -7.5360e-02, -2.2011e-01,\n",
       "           2.0602e-02, -3.8483e-01]],\n",
       "\n",
       "        [[ 2.0229e-01, -1.7108e-01,  2.5461e-01, -1.3252e-01, -2.5191e-02,\n",
       "           1.5870e-01, -3.7641e-01]],\n",
       "\n",
       "        [[ 3.0926e-01, -2.1614e-01,  3.8024e-01, -1.6892e-01, -5.0382e-02,\n",
       "           1.7229e-01, -3.4550e-01]],\n",
       "\n",
       "        [[ 3.6703e-01, -1.7108e-01,  4.2379e-01, -1.4816e-01,  7.5573e-02,\n",
       "           1.4467e-01, -3.6235e-01]],\n",
       "\n",
       "        [[ 3.1215e-01, -2.1614e-01,  4.0201e-01, -1.4816e-01, -8.7962e-02,\n",
       "           6.2259e-02, -3.8483e-01]],\n",
       "\n",
       "        [[ 3.6992e-01, -1.0814e-01,  4.6565e-01, -7.0243e-02, -1.0056e-01,\n",
       "           8.9880e-02, -3.9609e-01]],\n",
       "\n",
       "        [[ 4.4509e-01,  1.3517e-01,  5.2933e-01,  1.0650e-01, -2.2011e-01,\n",
       "           1.4467e-01, -4.0168e-01]],\n",
       "\n",
       "        [[ 5.6358e-01,  1.8924e-01,  5.6954e-01,  2.5196e-01,  2.9568e-01,\n",
       "           2.2753e-01, -4.1294e-01]],\n",
       "\n",
       "        [[ 6.5316e-01,  3.9637e-01,  6.6669e-01,  3.9215e-01,  2.5150e-01,\n",
       "           2.5515e-01, -4.1294e-01]],\n",
       "\n",
       "        [[ 6.3581e-01,  3.7835e-01,  6.4322e-01,  2.6745e-01,  2.5150e-01,\n",
       "           1.7229e-01, -4.1853e-01]],\n",
       "\n",
       "        [[ 6.3581e-01,  4.1439e-01,  6.4826e-01,  3.6101e-01,  2.8309e-01,\n",
       "           2.9635e-01, -4.1011e-01]],\n",
       "\n",
       "        [[ 6.0117e-01,  2.7935e-01,  6.2813e-01,  2.7783e-01,  2.0132e-01,\n",
       "           2.5515e-01, -4.0731e-01]],\n",
       "\n",
       "        [[ 5.7515e-01,  3.7835e-01,  6.1979e-01,  3.5063e-01,  2.3911e-01,\n",
       "           2.1395e-01, -4.0731e-01]],\n",
       "\n",
       "        [[ 5.2022e-01,  8.1103e-02,  5.6115e-01,  9.6119e-02,  2.0132e-01,\n",
       "           1.9991e-01, -4.3822e-01]],\n",
       "\n",
       "        [[ 3.8438e-01, -2.7034e-02,  4.9417e-01,  3.3696e-02, -1.8233e-01,\n",
       "           1.3108e-01, -4.1294e-01]],\n",
       "\n",
       "        [[ 6.0984e-01,  1.8924e-01,  5.7119e-01,  1.6877e-01,  4.9680e-01,\n",
       "           3.1039e-01, -4.2979e-01]],\n",
       "\n",
       "        [[ 4.3353e-01,  1.8023e-01,  5.3433e-01,  2.2593e-01, -1.7613e-01,\n",
       "           1.0346e-01, -4.1853e-01]],\n",
       "\n",
       "        [[ 3.1215e-01,  6.3080e-02,  4.0036e-01,  4.4076e-02, -8.7962e-02,\n",
       "           2.2753e-01, -4.0452e-01]],\n",
       "\n",
       "        [[ 1.7053e-01,  3.6046e-02,  2.4288e-01,  1.6351e-01, -1.1315e-01,\n",
       "           1.3108e-01, -4.0731e-01]],\n",
       "\n",
       "        [[ 1.2138e-01,  9.0114e-03,  1.9932e-01,  8.5593e-02, -1.1955e-01,\n",
       "           7.5843e-02, -4.0731e-01]],\n",
       "\n",
       "        [[-2.6019e-02, -2.0713e-01,  6.0313e-02, -9.6119e-02, -3.1447e-01,\n",
       "          -6.1807e-02, -4.0452e-01]]], device='cuda:0')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:34:52.442877Z",
     "start_time": "2023-03-16T07:34:52.383931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2983, -0.0355,  0.3375,  0.0510, -0.2153,  0.0699, -0.2547]],\n",
       "\n",
       "        [[ 0.3012, -0.0514,  0.3332,  0.0324, -0.1945,  0.0887, -0.2552]],\n",
       "\n",
       "        [[ 0.1760, -0.0142,  0.3162, -0.0471, -0.1468,  0.1006, -0.1968]],\n",
       "\n",
       "        [[ 0.3051, -0.0333,  0.3322,  0.0518, -0.2146,  0.0894, -0.2478]],\n",
       "\n",
       "        [[ 0.2999, -0.0396,  0.3479,  0.0578, -0.1986,  0.0675, -0.2547]],\n",
       "\n",
       "        [[ 0.3589, -0.0451,  0.2857,  0.0770, -0.2057,  0.1015, -0.2221]],\n",
       "\n",
       "        [[ 0.3209, -0.0427,  0.3326,  0.0706, -0.1991,  0.0758, -0.2470]],\n",
       "\n",
       "        [[ 0.2881, -0.0538,  0.3267,  0.0243, -0.2032,  0.0553, -0.2617]],\n",
       "\n",
       "        [[ 0.2871, -0.0446,  0.3212,  0.0077, -0.2130,  0.0928, -0.2539]],\n",
       "\n",
       "        [[ 0.2686, -0.0325,  0.3350,  0.0115, -0.2240,  0.1045, -0.2580]],\n",
       "\n",
       "        [[ 0.2599, -0.0454,  0.3348,  0.0015, -0.2157,  0.0698, -0.2680]],\n",
       "\n",
       "        [[ 0.3206, -0.0475,  0.3013,  0.0348, -0.2111,  0.1253, -0.2403]],\n",
       "\n",
       "        [[ 0.2975, -0.0372,  0.3278,  0.0337, -0.2163,  0.0797, -0.2512]],\n",
       "\n",
       "        [[ 0.2763, -0.0485,  0.3125, -0.0076, -0.2189,  0.0974, -0.2578]],\n",
       "\n",
       "        [[ 0.1849, -0.0363,  0.3349, -0.0700, -0.1640,  0.1205, -0.2370]],\n",
       "\n",
       "        [[ 0.2678, -0.0441,  0.3277, -0.0120, -0.1996,  0.1034, -0.2511]],\n",
       "\n",
       "        [[ 0.2093, -0.0210,  0.3027, -0.0465, -0.1756,  0.1068, -0.2081]],\n",
       "\n",
       "        [[ 0.3093, -0.0322,  0.3323,  0.0640, -0.2151,  0.1048, -0.2465]],\n",
       "\n",
       "        [[ 0.3126, -0.0414,  0.3124,  0.0406, -0.2202,  0.1101, -0.2470]],\n",
       "\n",
       "        [[ 0.2616, -0.0501,  0.3299, -0.0011, -0.2112,  0.0835, -0.2670]],\n",
       "\n",
       "        [[ 0.2454, -0.0524,  0.3420, -0.0136, -0.1983,  0.0840, -0.2704]],\n",
       "\n",
       "        [[ 0.2885, -0.0285,  0.3276,  0.0330, -0.2298,  0.1055, -0.2509]],\n",
       "\n",
       "        [[ 0.3101, -0.0336,  0.3055,  0.0396, -0.2341,  0.1130, -0.2436]],\n",
       "\n",
       "        [[ 0.3169, -0.0460,  0.3156,  0.0422, -0.2107,  0.0880, -0.2476]],\n",
       "\n",
       "        [[ 0.3008, -0.0310,  0.3339,  0.0541, -0.2162,  0.1104, -0.2476]],\n",
       "\n",
       "        [[ 0.3000, -0.0335,  0.3244,  0.0434, -0.2265,  0.1040, -0.2510]],\n",
       "\n",
       "        [[ 0.2752, -0.0351,  0.3471,  0.0261, -0.2131,  0.0884, -0.2605]],\n",
       "\n",
       "        [[ 0.2880, -0.0347,  0.3358,  0.0403, -0.2073,  0.1262, -0.2485]],\n",
       "\n",
       "        [[ 0.2568, -0.0259,  0.3501,  0.0108, -0.2189,  0.0890, -0.2583]],\n",
       "\n",
       "        [[ 0.2153, -0.0014,  0.3390, -0.0015, -0.1963,  0.0831, -0.2203]],\n",
       "\n",
       "        [[ 0.2700, -0.0253,  0.3374,  0.0168, -0.2236,  0.0955, -0.2517]],\n",
       "\n",
       "        [[ 0.2967, -0.0360,  0.3306,  0.0478, -0.2109,  0.1257, -0.2480]],\n",
       "\n",
       "        [[ 0.2505, -0.0637,  0.3369, -0.0218, -0.1831,  0.0663, -0.2697]],\n",
       "\n",
       "        [[ 0.2701, -0.0397,  0.3488,  0.0258, -0.1995,  0.1132, -0.2585]],\n",
       "\n",
       "        [[ 0.2722, -0.0300,  0.3480,  0.0231, -0.2126,  0.0837, -0.2559]],\n",
       "\n",
       "        [[ 0.2290, -0.0149,  0.3162, -0.0222, -0.2001,  0.0929, -0.2212]],\n",
       "\n",
       "        [[ 0.2560, -0.0341,  0.3449,  0.0141, -0.2252,  0.0803, -0.2682]],\n",
       "\n",
       "        [[ 0.3051, -0.0500,  0.3196,  0.0419, -0.1869,  0.1416, -0.2409]],\n",
       "\n",
       "        [[ 0.2619, -0.0265,  0.3578,  0.0287, -0.2198,  0.0811, -0.2636]],\n",
       "\n",
       "        [[ 0.2815, -0.0329,  0.3468,  0.0404, -0.2159,  0.0932, -0.2591]],\n",
       "\n",
       "        [[ 0.2552, -0.0456,  0.3654,  0.0033, -0.1776,  0.0585, -0.2632]],\n",
       "\n",
       "        [[ 0.3016, -0.0379,  0.3409,  0.0496, -0.2022,  0.0832, -0.2504]],\n",
       "\n",
       "        [[ 0.3160, -0.0467,  0.3212,  0.0546, -0.1865,  0.1352, -0.2368]],\n",
       "\n",
       "        [[ 0.3562, -0.0465,  0.3023,  0.0969, -0.1883,  0.1271, -0.2228]],\n",
       "\n",
       "        [[ 0.2908, -0.0317,  0.3478,  0.0528, -0.2110,  0.0940, -0.2545]],\n",
       "\n",
       "        [[ 0.2542, -0.0397,  0.3652,  0.0211, -0.2010,  0.0604, -0.2714]],\n",
       "\n",
       "        [[ 0.2011, -0.0360,  0.3398, -0.0507, -0.1464,  0.0490, -0.2284]],\n",
       "\n",
       "        [[ 0.2508, -0.0204,  0.3731,  0.0320, -0.1998,  0.0905, -0.2554]],\n",
       "\n",
       "        [[ 0.2919, -0.0316,  0.3522,  0.0558, -0.1922,  0.1006, -0.2455]],\n",
       "\n",
       "        [[ 0.2806, -0.0330,  0.3654,  0.0536, -0.1943,  0.0793, -0.2573]],\n",
       "\n",
       "        [[ 0.2762, -0.0186,  0.3540,  0.0511, -0.2236,  0.0888, -0.2540]],\n",
       "\n",
       "        [[ 0.2713, -0.0231,  0.3520,  0.0358, -0.2203,  0.0927, -0.2558]],\n",
       "\n",
       "        [[ 0.2792, -0.0251,  0.3644,  0.0600, -0.2080,  0.0697, -0.2572]],\n",
       "\n",
       "        [[ 0.2059, -0.0064,  0.3522, -0.0118, -0.1895,  0.0756, -0.2297]],\n",
       "\n",
       "        [[ 0.3031, -0.0265,  0.3412,  0.0655, -0.2121,  0.0856, -0.2449]],\n",
       "\n",
       "        [[ 0.2729, -0.0232,  0.3757,  0.0640, -0.1932,  0.0565, -0.2541]],\n",
       "\n",
       "        [[ 0.2044, -0.0100,  0.3528, -0.0245, -0.2034,  0.0837, -0.2414]],\n",
       "\n",
       "        [[ 0.2960, -0.0514,  0.3346,  0.0256, -0.1946,  0.0796, -0.2567]],\n",
       "\n",
       "        [[ 0.2878, -0.0416,  0.3510,  0.0383, -0.1952,  0.0648, -0.2577]],\n",
       "\n",
       "        [[ 0.2703, -0.0328,  0.3583,  0.0300, -0.2061,  0.0654, -0.2612]],\n",
       "\n",
       "        [[ 0.3174, -0.0522,  0.3315,  0.0480, -0.1811,  0.0894, -0.2452]],\n",
       "\n",
       "        [[ 0.2873, -0.0374,  0.3482,  0.0388, -0.2022,  0.0913, -0.2555]],\n",
       "\n",
       "        [[ 0.3019, -0.0362,  0.3260,  0.0435, -0.2224,  0.0940, -0.2522]],\n",
       "\n",
       "        [[ 0.3139, -0.0506,  0.3322,  0.0576, -0.1955,  0.0785, -0.2534]],\n",
       "\n",
       "        [[ 0.2785, -0.0438,  0.3624,  0.0384, -0.1858,  0.0535, -0.2622]],\n",
       "\n",
       "        [[ 0.2673, -0.0412,  0.3736,  0.0358, -0.1805,  0.0531, -0.2640]],\n",
       "\n",
       "        [[ 0.3261, -0.0611,  0.3223,  0.0498, -0.1769,  0.1006, -0.2455]],\n",
       "\n",
       "        [[ 0.3135, -0.0497,  0.3372,  0.0580, -0.1740,  0.1117, -0.2420]],\n",
       "\n",
       "        [[ 0.3283, -0.0467,  0.3213,  0.0593, -0.1896,  0.0995, -0.2376]],\n",
       "\n",
       "        [[ 0.3148, -0.0515,  0.3372,  0.0606, -0.1854,  0.0914, -0.2508]],\n",
       "\n",
       "        [[ 0.3307, -0.0652,  0.3229,  0.0535, -0.1695,  0.0786, -0.2456]],\n",
       "\n",
       "        [[ 0.2792, -0.0455,  0.3457,  0.0143, -0.1810,  0.0932, -0.2506]],\n",
       "\n",
       "        [[ 0.3335, -0.0434,  0.3316,  0.0914, -0.1615,  0.0969, -0.2235]],\n",
       "\n",
       "        [[ 0.2945, -0.0357,  0.3539,  0.0582, -0.1818,  0.1003, -0.2442]],\n",
       "\n",
       "        [[ 0.2967, -0.0243,  0.3432,  0.0602, -0.2139,  0.0812, -0.2456]],\n",
       "\n",
       "        [[ 0.2891, -0.0256,  0.3584,  0.0676, -0.2067,  0.0841, -0.2522]],\n",
       "\n",
       "        [[ 0.2777, -0.0209,  0.3655,  0.0613, -0.2070,  0.0747, -0.2531]],\n",
       "\n",
       "        [[ 0.2501, -0.0533,  0.3627,  0.0147, -0.1818,  0.0444, -0.2736]],\n",
       "\n",
       "        [[ 0.3244, -0.0537,  0.3391,  0.0683, -0.1584,  0.0761, -0.2373]],\n",
       "\n",
       "        [[ 0.2970, -0.0415,  0.3517,  0.0559, -0.1726,  0.1053, -0.2431]],\n",
       "\n",
       "        [[ 0.3093, -0.0355,  0.3282,  0.0505, -0.2070,  0.1051, -0.2424]],\n",
       "\n",
       "        [[ 0.3071, -0.0464,  0.3265,  0.0382, -0.2056,  0.0884, -0.2522]],\n",
       "\n",
       "        [[ 0.2477, -0.0591,  0.3510, -0.0149, -0.1144,  0.0684, -0.2329]],\n",
       "\n",
       "        [[ 0.1877, -0.0332,  0.3416, -0.0607, -0.1485,  0.0569, -0.2293]],\n",
       "\n",
       "        [[ 0.1920, -0.0221,  0.3410, -0.0463, -0.1636,  0.0663, -0.2259]],\n",
       "\n",
       "        [[ 0.2985, -0.0442,  0.3300,  0.0277, -0.2029,  0.0865, -0.2512]],\n",
       "\n",
       "        [[ 0.3108, -0.0556,  0.3318,  0.0344, -0.1705,  0.0821, -0.2434]],\n",
       "\n",
       "        [[ 0.3090, -0.0409,  0.3374,  0.0570, -0.2014,  0.0719, -0.2505]],\n",
       "\n",
       "        [[ 0.3160, -0.0587,  0.3313,  0.0439, -0.1783,  0.0814, -0.2504]],\n",
       "\n",
       "        [[ 0.3100, -0.0550,  0.3231,  0.0275, -0.1898,  0.0882, -0.2491]],\n",
       "\n",
       "        [[ 0.2999, -0.0584,  0.3349,  0.0214, -0.1756,  0.0817, -0.2522]],\n",
       "\n",
       "        [[ 0.3080, -0.0555,  0.3114,  0.0221, -0.1800,  0.1361, -0.2370]],\n",
       "\n",
       "        [[ 0.3104, -0.0494,  0.3096,  0.0219, -0.2000,  0.1167, -0.2414]],\n",
       "\n",
       "        [[ 0.3093, -0.0398,  0.3194,  0.0424, -0.2162,  0.1108, -0.2472]],\n",
       "\n",
       "        [[ 0.3199, -0.0495,  0.3184,  0.0457, -0.2030,  0.0932, -0.2477]],\n",
       "\n",
       "        [[ 0.2976, -0.0642,  0.3242,  0.0148, -0.1696,  0.1203, -0.2484]],\n",
       "\n",
       "        [[ 0.2487, -0.0513,  0.3355, -0.0179, -0.2053,  0.0714, -0.2695]],\n",
       "\n",
       "        [[ 0.3070, -0.0449,  0.2996,  0.0162, -0.2161,  0.1226, -0.2411]],\n",
       "\n",
       "        [[ 0.2203, -0.0104,  0.3352, -0.0070, -0.1815,  0.0911, -0.2177]],\n",
       "\n",
       "        [[ 0.2842, -0.0392,  0.3378,  0.0246, -0.2126,  0.0865, -0.2581]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy = model(data)\n",
    "yyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:34:54.649464Z",
     "start_time": "2023-03-16T07:34:54.634540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 7])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:34:55.496346Z",
     "start_time": "2023-03-16T07:34:55.471189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2842, -0.0392,  0.3378,  0.0246, -0.2126,  0.0865, -0.2581]]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:35:08.498456Z",
     "start_time": "2023-03-16T07:35:08.467508Z"
    }
   },
   "outputs": [],
   "source": [
    "data11 = torch.cat((data, yyy[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:35:57.161019Z",
     "start_time": "2023-03-16T07:35:57.144078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 1, 7])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:35:11.787173Z",
     "start_time": "2023-03-16T07:35:11.730323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.8324e-01, -1.7108e-01, -2.7304e-01, -7.0243e-02, -1.5094e-01,\n",
       "           1.1750e-01, -6.1519e-01]],\n",
       "\n",
       "        [[-5.5780e-01,  1.1715e-01, -5.7284e-01,  1.8442e-01, -6.2978e-02,\n",
       "           1.9991e-01, -5.5901e-01]],\n",
       "\n",
       "        [[-6.6188e-01,  1.3517e-01, -7.2528e-01,  1.6877e-01, -1.2595e-02,\n",
       "           1.4467e-01, -5.7865e-01]],\n",
       "\n",
       "        [[-7.8326e-01,  9.0114e-02, -8.2243e-01,  4.9338e-02, -2.2011e-01,\n",
       "           6.2259e-02, -5.7586e-01]],\n",
       "\n",
       "        [[-7.9771e-01,  1.9825e-01, -7.7888e-01,  1.6351e-01, -2.3890e-01,\n",
       "           4.8223e-02, -5.4775e-01]],\n",
       "\n",
       "        [[-7.3989e-01,  1.2616e-01, -7.5880e-01,  1.5313e-01, -2.7049e-01,\n",
       "           8.9880e-02, -4.6349e-01]],\n",
       "\n",
       "        [[-5.1159e-01,  6.3080e-02, -5.9966e-01,  6.4834e-02,  2.7049e-01,\n",
       "           1.7229e-01, -4.9719e-01]],\n",
       "\n",
       "        [[-2.1679e-01, -1.0814e-01, -2.7974e-01, -9.1002e-02,  3.1447e-01,\n",
       "           2.2753e-01, -4.6349e-01]],\n",
       "\n",
       "        [[ 9.5361e-02, -2.0713e-01,  1.5916e-01, -1.2726e-01, -8.1768e-02,\n",
       "           1.3108e-01, -4.4944e-01]],\n",
       "\n",
       "        [[ 3.3238e-01, -1.2616e-01,  4.1875e-01, -5.4601e-02, -8.1768e-02,\n",
       "          -2.0602e-02, -4.8314e-01]],\n",
       "\n",
       "        [[ 3.7571e-01, -1.6207e-01,  4.5226e-01, -7.5360e-02, -5.0382e-02,\n",
       "          -4.8223e-02, -5.1684e-01]],\n",
       "\n",
       "        [[ 3.7282e-01, -1.5306e-01,  4.4891e-01, -7.5360e-02, -5.0382e-02,\n",
       "          -3.4639e-02, -5.5617e-01]],\n",
       "\n",
       "        [[ 3.7860e-01, -1.4405e-01,  4.5396e-01, -1.2214e-01, -5.0382e-02,\n",
       "           4.8223e-02, -5.6460e-01]],\n",
       "\n",
       "        [[ 4.6244e-01,  1.8023e-02,  5.6450e-01,  1.3237e-01, -1.3215e-01,\n",
       "           3.4639e-02, -5.7302e-01]],\n",
       "\n",
       "        [[ 6.2429e-01,  2.8837e-01,  6.7004e-01,  2.2067e-01,  3.5226e-01,\n",
       "           6.2780e-01, -5.8145e-01]],\n",
       "\n",
       "        [[ 6.7051e-01,  5.0451e-01,  7.3033e-01,  4.0779e-01,  1.9492e-01,\n",
       "           1.3108e-01, -5.7023e-01]],\n",
       "\n",
       "        [[ 7.1387e-01,  4.0538e-01,  7.6049e-01,  3.4537e-01,  2.0752e-01,\n",
       "           1.3108e-01, -5.6460e-01]],\n",
       "\n",
       "        [[ 5.9249e-01,  1.8023e-01,  6.3652e-01,  2.1555e-01,  2.1392e-01,\n",
       "           2.1395e-01, -5.6180e-01]],\n",
       "\n",
       "        [[ 6.0117e-01,  1.9825e-01,  6.4322e-01,  1.9480e-01,  1.5094e-01,\n",
       "           1.1750e-01, -5.6180e-01]],\n",
       "\n",
       "        [[ 5.7804e-01,  1.3517e-01,  6.0805e-01,  1.9991e-01,  2.5790e-01,\n",
       "           1.4467e-01, -5.8428e-01]],\n",
       "\n",
       "        [[ 5.7804e-01,  2.1627e-01,  6.0805e-01,  1.2726e-01,  2.6409e-01,\n",
       "           1.9991e-01, -5.8428e-01]],\n",
       "\n",
       "        [[ 6.5316e-01,  2.1627e-01,  6.6499e-01,  1.4275e-01,  3.3326e-01,\n",
       "           2.5515e-01, -5.7302e-01]],\n",
       "\n",
       "        [[ 6.7629e-01,  2.7935e-01,  6.6834e-01,  2.3631e-01,  5.4718e-01,\n",
       "           3.5160e-01, -6.1235e-01]],\n",
       "\n",
       "        [[ 1.2138e-01,  1.4418e-01,  1.7589e-01,  2.4158e-01, -3.7787e-02,\n",
       "           8.9880e-02, -6.0393e-01]],\n",
       "\n",
       "        [[-2.7456e-01,  8.1103e-02, -2.3284e-01,  1.5313e-01, -1.1315e-01,\n",
       "           1.3108e-01, -6.0956e-01]],\n",
       "\n",
       "        [[-5.1159e-01,  2.2529e-01, -4.7739e-01,  3.1423e-01, -1.5094e-01,\n",
       "           1.4467e-01, -5.5617e-01]],\n",
       "\n",
       "        [[-6.4742e-01,  1.1715e-01, -6.5491e-01, -4.4222e-02, -8.7962e-02,\n",
       "           6.2259e-02, -5.7023e-01]],\n",
       "\n",
       "        [[-7.8904e-01,  5.4069e-02, -7.8727e-01, -9.6119e-02, -2.7049e-01,\n",
       "          -2.0602e-02, -5.2810e-01]],\n",
       "\n",
       "        [[-6.5031e-01, -3.2067e-08, -6.2144e-01,  8.5593e-02, -2.9548e-01,\n",
       "           7.0183e-03, -5.0562e-01]],\n",
       "\n",
       "        [[-6.3008e-01,  1.8924e-01, -7.0685e-01,  9.6119e-02,  2.5150e-01,\n",
       "           2.8277e-01, -4.2416e-01]],\n",
       "\n",
       "        [[-3.8438e-01,  4.5057e-02, -4.5895e-01, -2.7045e-03,  2.8928e-01,\n",
       "           2.6919e-01, -4.4101e-01]],\n",
       "\n",
       "        [[ 6.6451e-02, -1.4405e-01,  4.1882e-02, -1.1688e-01,  3.0188e-01,\n",
       "           2.5515e-01, -4.4944e-01]],\n",
       "\n",
       "        [[ 1.9077e-01, -2.0713e-01,  2.5631e-01, -1.2214e-01, -7.5367e-02,\n",
       "          -7.0183e-03, -4.4944e-01]],\n",
       "\n",
       "        [[ 3.0926e-01, -2.1614e-01,  3.9197e-01, -1.5328e-01, -1.0056e-01,\n",
       "           7.0183e-03, -4.8034e-01]],\n",
       "\n",
       "        [[ 3.6414e-01, -2.1614e-01,  4.4052e-01, -1.3778e-01, -5.0382e-02,\n",
       "          -7.0183e-03, -4.9719e-01]],\n",
       "\n",
       "        [[ 3.3238e-01, -2.5219e-01,  4.1540e-01, -1.6892e-01, -1.0056e-01,\n",
       "          -3.4639e-02, -5.0283e-01]],\n",
       "\n",
       "        [[ 3.8438e-01, -9.0114e-02,  4.7908e-01, -1.8200e-02, -1.2575e-01,\n",
       "           3.4639e-02, -5.2247e-01]],\n",
       "\n",
       "        [[ 4.2775e-01,  1.7122e-01,  5.4941e-01,  1.5313e-01, -2.2631e-01,\n",
       "           7.0183e-03, -5.3653e-01]],\n",
       "\n",
       "        [[ 5.9249e-01,  3.3329e-01,  7.1025e-01,  3.7651e-01, -1.6354e-01,\n",
       "           4.8223e-02, -5.3090e-01]],\n",
       "\n",
       "        [[ 5.7515e-01,  4.5044e-01,  5.9966e-01,  3.5575e-01,  3.3326e-01,\n",
       "           2.2753e-01, -5.1684e-01]],\n",
       "\n",
       "        [[ 5.4913e-01,  4.0538e-01,  5.7454e-01,  4.0779e-01,  2.2651e-01,\n",
       "           2.6919e-01, -5.1968e-01]],\n",
       "\n",
       "        [[ 5.6936e-01,  3.6933e-01,  6.1474e-01,  3.7139e-01,  1.2575e-01,\n",
       "           1.0346e-01, -5.3373e-01]],\n",
       "\n",
       "        [[ 5.3756e-01,  2.9738e-01,  5.8123e-01,  3.4025e-01,  1.3215e-01,\n",
       "           8.9880e-02, -5.3090e-01]],\n",
       "\n",
       "        [[ 5.5491e-01,  4.4143e-01,  5.8123e-01,  4.0253e-01,  1.4474e-01,\n",
       "           1.3108e-01, -5.1968e-01]],\n",
       "\n",
       "        [[ 5.2311e-01,  2.9738e-01,  5.5276e-01,  2.7271e-01,  2.3271e-01,\n",
       "           1.9991e-01, -5.1125e-01]],\n",
       "\n",
       "        [[ 5.2311e-01,  7.2091e-02,  5.6280e-01,  7.5214e-02,  3.2067e-01,\n",
       "           2.9635e-01, -5.3090e-01]],\n",
       "\n",
       "        [[ 6.0117e-01,  2.4331e-01,  5.5446e-01,  1.2726e-01,  4.5922e-01,\n",
       "           1.9991e-01, -5.3932e-01]],\n",
       "\n",
       "        [[ 1.0982e-01,  6.3080e-02,  1.6750e-01,  1.4275e-01, -1.1955e-01,\n",
       "           4.8223e-02, -5.1684e-01]],\n",
       "\n",
       "        [[-3.1215e-01, -1.1715e-01, -2.5127e-01, -3.8959e-02, -1.5713e-01,\n",
       "           1.0346e-01, -5.0562e-01]],\n",
       "\n",
       "        [[-5.3756e-01,  5.4069e-02, -5.2594e-01,  1.2938e-02, -1.8873e-01,\n",
       "           1.5870e-01, -4.8314e-01]],\n",
       "\n",
       "        [[-6.4164e-01,  4.5057e-02, -6.3652e-01,  4.9338e-02, -1.3215e-01,\n",
       "           3.4639e-02, -4.8877e-01]],\n",
       "\n",
       "        [[-7.9193e-01,  5.4069e-02, -7.7553e-01,  5.4455e-02, -3.3326e-01,\n",
       "          -7.5843e-02, -4.7755e-01]],\n",
       "\n",
       "        [[-6.8207e-01,  3.8736e-01, -7.6714e-01,  1.9991e-01,  1.9492e-01,\n",
       "           5.5852e-01, -4.6349e-01]],\n",
       "\n",
       "        [[-6.2140e-01,  2.7034e-02, -7.2194e-01, -2.7045e-03,  2.2651e-01,\n",
       "           2.5515e-01, -4.1574e-01]],\n",
       "\n",
       "        [[-4.4509e-01, -5.4069e-02, -5.4772e-01, -9.1002e-02,  2.4530e-01,\n",
       "           2.9635e-01, -4.3259e-01]],\n",
       "\n",
       "        [[-2.5722e-01, -2.0713e-01, -2.8473e-01, -1.8968e-01,  2.7049e-01,\n",
       "           2.8277e-01, -4.1853e-01]],\n",
       "\n",
       "        [[ 2.3128e-02, -3.0625e-01,  1.3738e-01, -3.0400e-01, -1.2575e-01,\n",
       "           3.4639e-02, -3.9046e-01]],\n",
       "\n",
       "        [[ 2.9769e-01, -3.8736e-01,  3.9362e-01, -3.1438e-01, -1.6354e-01,\n",
       "          -6.1807e-02, -3.9889e-01]],\n",
       "\n",
       "        [[ 3.5258e-01, -3.6032e-01,  4.2549e-01, -3.2476e-01, -6.1945e-03,\n",
       "           6.2259e-02, -4.4101e-01]],\n",
       "\n",
       "        [[ 3.4391e-01, -3.6032e-01,  4.1375e-01, -3.1949e-01, -3.7787e-02,\n",
       "           1.1750e-01, -4.4381e-01]],\n",
       "\n",
       "        [[ 2.9769e-01, -3.3329e-01,  4.1040e-01, -2.5722e-01, -1.6973e-01,\n",
       "           6.2259e-02, -4.6070e-01]],\n",
       "\n",
       "        [[ 4.0173e-01, -7.2091e-02,  5.3602e-01,  3.3696e-02, -2.5150e-01,\n",
       "          -3.4639e-02, -4.6629e-01]],\n",
       "\n",
       "        [[ 6.4738e-01,  2.1627e-01,  6.7838e-01,  2.3631e-01,  2.2011e-01,\n",
       "           1.7229e-01, -4.6912e-01]],\n",
       "\n",
       "        [[ 6.5894e-01,  3.6933e-01,  6.9681e-01,  2.7783e-01,  1.6353e-01,\n",
       "           8.9880e-02, -4.7755e-01]],\n",
       "\n",
       "        [[ 6.6183e-01,  4.1439e-01,  7.0520e-01,  3.7651e-01,  1.8233e-01,\n",
       "           1.1750e-01, -4.8877e-01]],\n",
       "\n",
       "        [[ 5.5780e-01,  2.6133e-01,  6.1809e-01,  2.6745e-01,  1.0696e-01,\n",
       "           4.8223e-02, -4.8314e-01]],\n",
       "\n",
       "        [[ 5.9249e-01,  3.2428e-01,  6.6834e-01,  3.7139e-01,  1.1955e-01,\n",
       "           8.9880e-02, -4.8597e-01]],\n",
       "\n",
       "        [[ 5.6647e-01,  3.6032e-01,  6.3318e-01,  2.8821e-01,  1.4474e-01,\n",
       "           1.4467e-01, -4.8877e-01]],\n",
       "\n",
       "        [[ 6.6183e-01,  4.1439e-01,  7.0685e-01,  3.9215e-01,  2.3271e-01,\n",
       "           1.3108e-01, -4.8877e-01]],\n",
       "\n",
       "        [[ 6.5894e-01,  3.0639e-01,  6.4157e-01,  2.6233e-01,  3.3326e-01,\n",
       "           3.1039e-01, -4.9440e-01]],\n",
       "\n",
       "        [[ 5.6647e-01,  1.3517e-01,  5.4442e-01,  5.9718e-02,  4.1503e-01,\n",
       "           2.2753e-01, -5.0842e-01]],\n",
       "\n",
       "        [[ 5.1996e-02,  2.7034e-02,  1.4573e-01,  8.0477e-02, -1.2575e-01,\n",
       "          -7.0183e-03, -5.1125e-01]],\n",
       "\n",
       "        [[-3.2371e-01, -1.2616e-01, -2.8139e-01, -1.3084e-02, -2.0132e-01,\n",
       "           6.2259e-02, -4.6629e-01]],\n",
       "\n",
       "        [[-4.9135e-01,  8.1103e-02, -4.9582e-01,  1.4275e-01, -1.5713e-01,\n",
       "           6.2259e-02, -3.9889e-01]],\n",
       "\n",
       "        [[-6.0695e-01,  1.2616e-01, -6.0635e-01,  1.0650e-01, -1.3215e-01,\n",
       "           1.0346e-01, -4.0452e-01]],\n",
       "\n",
       "        [[-7.7458e-01, -5.4069e-02, -7.7723e-01,  1.8200e-02, -2.9548e-01,\n",
       "          -7.0183e-03, -4.0168e-01]],\n",
       "\n",
       "        [[-7.4278e-01,  6.3080e-02, -7.1020e-01,  9.0856e-02, -3.1447e-01,\n",
       "          -7.0183e-03, -3.7361e-01]],\n",
       "\n",
       "        [[-6.2140e-01,  4.5057e-02, -5.6784e-01,  1.3237e-01, -3.1447e-01,\n",
       "           7.5843e-02, -3.3148e-01]],\n",
       "\n",
       "        [[-4.3353e-01, -7.2091e-02, -3.9028e-01,  7.0097e-02, -3.4586e-01,\n",
       "           6.2259e-02, -3.6798e-01]],\n",
       "\n",
       "        [[-1.7920e-01, -1.8911e-01, -9.2130e-02, -7.5360e-02, -2.2011e-01,\n",
       "           2.0602e-02, -3.8483e-01]],\n",
       "\n",
       "        [[ 2.0229e-01, -1.7108e-01,  2.5461e-01, -1.3252e-01, -2.5191e-02,\n",
       "           1.5870e-01, -3.7641e-01]],\n",
       "\n",
       "        [[ 3.0926e-01, -2.1614e-01,  3.8024e-01, -1.6892e-01, -5.0382e-02,\n",
       "           1.7229e-01, -3.4550e-01]],\n",
       "\n",
       "        [[ 3.6703e-01, -1.7108e-01,  4.2379e-01, -1.4816e-01,  7.5573e-02,\n",
       "           1.4467e-01, -3.6235e-01]],\n",
       "\n",
       "        [[ 3.1215e-01, -2.1614e-01,  4.0201e-01, -1.4816e-01, -8.7962e-02,\n",
       "           6.2259e-02, -3.8483e-01]],\n",
       "\n",
       "        [[ 3.6992e-01, -1.0814e-01,  4.6565e-01, -7.0243e-02, -1.0056e-01,\n",
       "           8.9880e-02, -3.9609e-01]],\n",
       "\n",
       "        [[ 4.4509e-01,  1.3517e-01,  5.2933e-01,  1.0650e-01, -2.2011e-01,\n",
       "           1.4467e-01, -4.0168e-01]],\n",
       "\n",
       "        [[ 5.6358e-01,  1.8924e-01,  5.6954e-01,  2.5196e-01,  2.9568e-01,\n",
       "           2.2753e-01, -4.1294e-01]],\n",
       "\n",
       "        [[ 6.5316e-01,  3.9637e-01,  6.6669e-01,  3.9215e-01,  2.5150e-01,\n",
       "           2.5515e-01, -4.1294e-01]],\n",
       "\n",
       "        [[ 6.3581e-01,  3.7835e-01,  6.4322e-01,  2.6745e-01,  2.5150e-01,\n",
       "           1.7229e-01, -4.1853e-01]],\n",
       "\n",
       "        [[ 6.3581e-01,  4.1439e-01,  6.4826e-01,  3.6101e-01,  2.8309e-01,\n",
       "           2.9635e-01, -4.1011e-01]],\n",
       "\n",
       "        [[ 6.0117e-01,  2.7935e-01,  6.2813e-01,  2.7783e-01,  2.0132e-01,\n",
       "           2.5515e-01, -4.0731e-01]],\n",
       "\n",
       "        [[ 5.7515e-01,  3.7835e-01,  6.1979e-01,  3.5063e-01,  2.3911e-01,\n",
       "           2.1395e-01, -4.0731e-01]],\n",
       "\n",
       "        [[ 5.2022e-01,  8.1103e-02,  5.6115e-01,  9.6119e-02,  2.0132e-01,\n",
       "           1.9991e-01, -4.3822e-01]],\n",
       "\n",
       "        [[ 3.8438e-01, -2.7034e-02,  4.9417e-01,  3.3696e-02, -1.8233e-01,\n",
       "           1.3108e-01, -4.1294e-01]],\n",
       "\n",
       "        [[ 6.0984e-01,  1.8924e-01,  5.7119e-01,  1.6877e-01,  4.9680e-01,\n",
       "           3.1039e-01, -4.2979e-01]],\n",
       "\n",
       "        [[ 4.3353e-01,  1.8023e-01,  5.3433e-01,  2.2593e-01, -1.7613e-01,\n",
       "           1.0346e-01, -4.1853e-01]],\n",
       "\n",
       "        [[ 3.1215e-01,  6.3080e-02,  4.0036e-01,  4.4076e-02, -8.7962e-02,\n",
       "           2.2753e-01, -4.0452e-01]],\n",
       "\n",
       "        [[ 1.7053e-01,  3.6046e-02,  2.4288e-01,  1.6351e-01, -1.1315e-01,\n",
       "           1.3108e-01, -4.0731e-01]],\n",
       "\n",
       "        [[ 1.2138e-01,  9.0114e-03,  1.9932e-01,  8.5593e-02, -1.1955e-01,\n",
       "           7.5843e-02, -4.0731e-01]],\n",
       "\n",
       "        [[-2.6019e-02, -2.0713e-01,  6.0313e-02, -9.6119e-02, -3.1447e-01,\n",
       "          -6.1807e-02, -4.0452e-01]],\n",
       "\n",
       "        [[ 2.8425e-01, -3.9217e-02,  3.3779e-01,  2.4600e-02, -2.1262e-01,\n",
       "           8.6527e-02, -2.5807e-01]]], device='cuda:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:09:54.260609Z",
     "start_time": "2023-03-16T07:09:54.251637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 7])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-input_window:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:27:26.323192Z",
     "start_time": "2023-03-16T07:27:26.264903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.8324e-01, -1.7108e-01, -2.7304e-01, -7.0243e-02, -1.5094e-01,\n",
       "           1.1750e-01, -6.1519e-01]],\n",
       "\n",
       "        [[-5.5780e-01,  1.1715e-01, -5.7284e-01,  1.8442e-01, -6.2978e-02,\n",
       "           1.9991e-01, -5.5901e-01]],\n",
       "\n",
       "        [[-6.6188e-01,  1.3517e-01, -7.2528e-01,  1.6877e-01, -1.2595e-02,\n",
       "           1.4467e-01, -5.7865e-01]],\n",
       "\n",
       "        [[-7.8326e-01,  9.0114e-02, -8.2243e-01,  4.9338e-02, -2.2011e-01,\n",
       "           6.2259e-02, -5.7586e-01]],\n",
       "\n",
       "        [[-7.9771e-01,  1.9825e-01, -7.7888e-01,  1.6351e-01, -2.3890e-01,\n",
       "           4.8223e-02, -5.4775e-01]],\n",
       "\n",
       "        [[-7.3989e-01,  1.2616e-01, -7.5880e-01,  1.5313e-01, -2.7049e-01,\n",
       "           8.9880e-02, -4.6349e-01]],\n",
       "\n",
       "        [[-5.1159e-01,  6.3080e-02, -5.9966e-01,  6.4834e-02,  2.7049e-01,\n",
       "           1.7229e-01, -4.9719e-01]],\n",
       "\n",
       "        [[-2.1679e-01, -1.0814e-01, -2.7974e-01, -9.1002e-02,  3.1447e-01,\n",
       "           2.2753e-01, -4.6349e-01]],\n",
       "\n",
       "        [[ 9.5361e-02, -2.0713e-01,  1.5916e-01, -1.2726e-01, -8.1768e-02,\n",
       "           1.3108e-01, -4.4944e-01]],\n",
       "\n",
       "        [[ 3.3238e-01, -1.2616e-01,  4.1875e-01, -5.4601e-02, -8.1768e-02,\n",
       "          -2.0602e-02, -4.8314e-01]],\n",
       "\n",
       "        [[ 3.7571e-01, -1.6207e-01,  4.5226e-01, -7.5360e-02, -5.0382e-02,\n",
       "          -4.8223e-02, -5.1684e-01]],\n",
       "\n",
       "        [[ 3.7282e-01, -1.5306e-01,  4.4891e-01, -7.5360e-02, -5.0382e-02,\n",
       "          -3.4639e-02, -5.5617e-01]],\n",
       "\n",
       "        [[ 3.7860e-01, -1.4405e-01,  4.5396e-01, -1.2214e-01, -5.0382e-02,\n",
       "           4.8223e-02, -5.6460e-01]],\n",
       "\n",
       "        [[ 4.6244e-01,  1.8023e-02,  5.6450e-01,  1.3237e-01, -1.3215e-01,\n",
       "           3.4639e-02, -5.7302e-01]],\n",
       "\n",
       "        [[ 6.2429e-01,  2.8837e-01,  6.7004e-01,  2.2067e-01,  3.5226e-01,\n",
       "           6.2780e-01, -5.8145e-01]],\n",
       "\n",
       "        [[ 6.7051e-01,  5.0451e-01,  7.3033e-01,  4.0779e-01,  1.9492e-01,\n",
       "           1.3108e-01, -5.7023e-01]],\n",
       "\n",
       "        [[ 7.1387e-01,  4.0538e-01,  7.6049e-01,  3.4537e-01,  2.0752e-01,\n",
       "           1.3108e-01, -5.6460e-01]],\n",
       "\n",
       "        [[ 5.9249e-01,  1.8023e-01,  6.3652e-01,  2.1555e-01,  2.1392e-01,\n",
       "           2.1395e-01, -5.6180e-01]],\n",
       "\n",
       "        [[ 6.0117e-01,  1.9825e-01,  6.4322e-01,  1.9480e-01,  1.5094e-01,\n",
       "           1.1750e-01, -5.6180e-01]],\n",
       "\n",
       "        [[ 5.7804e-01,  1.3517e-01,  6.0805e-01,  1.9991e-01,  2.5790e-01,\n",
       "           1.4467e-01, -5.8428e-01]],\n",
       "\n",
       "        [[ 5.7804e-01,  2.1627e-01,  6.0805e-01,  1.2726e-01,  2.6409e-01,\n",
       "           1.9991e-01, -5.8428e-01]],\n",
       "\n",
       "        [[ 6.5316e-01,  2.1627e-01,  6.6499e-01,  1.4275e-01,  3.3326e-01,\n",
       "           2.5515e-01, -5.7302e-01]],\n",
       "\n",
       "        [[ 6.7629e-01,  2.7935e-01,  6.6834e-01,  2.3631e-01,  5.4718e-01,\n",
       "           3.5160e-01, -6.1235e-01]],\n",
       "\n",
       "        [[ 1.2138e-01,  1.4418e-01,  1.7589e-01,  2.4158e-01, -3.7787e-02,\n",
       "           8.9880e-02, -6.0393e-01]],\n",
       "\n",
       "        [[-2.7456e-01,  8.1103e-02, -2.3284e-01,  1.5313e-01, -1.1315e-01,\n",
       "           1.3108e-01, -6.0956e-01]],\n",
       "\n",
       "        [[-5.1159e-01,  2.2529e-01, -4.7739e-01,  3.1423e-01, -1.5094e-01,\n",
       "           1.4467e-01, -5.5617e-01]],\n",
       "\n",
       "        [[-6.4742e-01,  1.1715e-01, -6.5491e-01, -4.4222e-02, -8.7962e-02,\n",
       "           6.2259e-02, -5.7023e-01]],\n",
       "\n",
       "        [[-7.8904e-01,  5.4069e-02, -7.8727e-01, -9.6119e-02, -2.7049e-01,\n",
       "          -2.0602e-02, -5.2810e-01]],\n",
       "\n",
       "        [[-6.5031e-01, -3.2067e-08, -6.2144e-01,  8.5593e-02, -2.9548e-01,\n",
       "           7.0183e-03, -5.0562e-01]],\n",
       "\n",
       "        [[-6.3008e-01,  1.8924e-01, -7.0685e-01,  9.6119e-02,  2.5150e-01,\n",
       "           2.8277e-01, -4.2416e-01]],\n",
       "\n",
       "        [[-3.8438e-01,  4.5057e-02, -4.5895e-01, -2.7045e-03,  2.8928e-01,\n",
       "           2.6919e-01, -4.4101e-01]],\n",
       "\n",
       "        [[ 6.6451e-02, -1.4405e-01,  4.1882e-02, -1.1688e-01,  3.0188e-01,\n",
       "           2.5515e-01, -4.4944e-01]],\n",
       "\n",
       "        [[ 1.9077e-01, -2.0713e-01,  2.5631e-01, -1.2214e-01, -7.5367e-02,\n",
       "          -7.0183e-03, -4.4944e-01]],\n",
       "\n",
       "        [[ 3.0926e-01, -2.1614e-01,  3.9197e-01, -1.5328e-01, -1.0056e-01,\n",
       "           7.0183e-03, -4.8034e-01]],\n",
       "\n",
       "        [[ 3.6414e-01, -2.1614e-01,  4.4052e-01, -1.3778e-01, -5.0382e-02,\n",
       "          -7.0183e-03, -4.9719e-01]],\n",
       "\n",
       "        [[ 3.3238e-01, -2.5219e-01,  4.1540e-01, -1.6892e-01, -1.0056e-01,\n",
       "          -3.4639e-02, -5.0283e-01]],\n",
       "\n",
       "        [[ 3.8438e-01, -9.0114e-02,  4.7908e-01, -1.8200e-02, -1.2575e-01,\n",
       "           3.4639e-02, -5.2247e-01]],\n",
       "\n",
       "        [[ 4.2775e-01,  1.7122e-01,  5.4941e-01,  1.5313e-01, -2.2631e-01,\n",
       "           7.0183e-03, -5.3653e-01]],\n",
       "\n",
       "        [[ 5.9249e-01,  3.3329e-01,  7.1025e-01,  3.7651e-01, -1.6354e-01,\n",
       "           4.8223e-02, -5.3090e-01]],\n",
       "\n",
       "        [[ 5.7515e-01,  4.5044e-01,  5.9966e-01,  3.5575e-01,  3.3326e-01,\n",
       "           2.2753e-01, -5.1684e-01]],\n",
       "\n",
       "        [[ 5.4913e-01,  4.0538e-01,  5.7454e-01,  4.0779e-01,  2.2651e-01,\n",
       "           2.6919e-01, -5.1968e-01]],\n",
       "\n",
       "        [[ 5.6936e-01,  3.6933e-01,  6.1474e-01,  3.7139e-01,  1.2575e-01,\n",
       "           1.0346e-01, -5.3373e-01]],\n",
       "\n",
       "        [[ 5.3756e-01,  2.9738e-01,  5.8123e-01,  3.4025e-01,  1.3215e-01,\n",
       "           8.9880e-02, -5.3090e-01]],\n",
       "\n",
       "        [[ 5.5491e-01,  4.4143e-01,  5.8123e-01,  4.0253e-01,  1.4474e-01,\n",
       "           1.3108e-01, -5.1968e-01]],\n",
       "\n",
       "        [[ 5.2311e-01,  2.9738e-01,  5.5276e-01,  2.7271e-01,  2.3271e-01,\n",
       "           1.9991e-01, -5.1125e-01]],\n",
       "\n",
       "        [[ 5.2311e-01,  7.2091e-02,  5.6280e-01,  7.5214e-02,  3.2067e-01,\n",
       "           2.9635e-01, -5.3090e-01]],\n",
       "\n",
       "        [[ 6.0117e-01,  2.4331e-01,  5.5446e-01,  1.2726e-01,  4.5922e-01,\n",
       "           1.9991e-01, -5.3932e-01]],\n",
       "\n",
       "        [[ 1.0982e-01,  6.3080e-02,  1.6750e-01,  1.4275e-01, -1.1955e-01,\n",
       "           4.8223e-02, -5.1684e-01]],\n",
       "\n",
       "        [[-3.1215e-01, -1.1715e-01, -2.5127e-01, -3.8959e-02, -1.5713e-01,\n",
       "           1.0346e-01, -5.0562e-01]],\n",
       "\n",
       "        [[-5.3756e-01,  5.4069e-02, -5.2594e-01,  1.2938e-02, -1.8873e-01,\n",
       "           1.5870e-01, -4.8314e-01]],\n",
       "\n",
       "        [[-6.4164e-01,  4.5057e-02, -6.3652e-01,  4.9338e-02, -1.3215e-01,\n",
       "           3.4639e-02, -4.8877e-01]],\n",
       "\n",
       "        [[-7.9193e-01,  5.4069e-02, -7.7553e-01,  5.4455e-02, -3.3326e-01,\n",
       "          -7.5843e-02, -4.7755e-01]],\n",
       "\n",
       "        [[-6.8207e-01,  3.8736e-01, -7.6714e-01,  1.9991e-01,  1.9492e-01,\n",
       "           5.5852e-01, -4.6349e-01]],\n",
       "\n",
       "        [[-6.2140e-01,  2.7034e-02, -7.2194e-01, -2.7045e-03,  2.2651e-01,\n",
       "           2.5515e-01, -4.1574e-01]],\n",
       "\n",
       "        [[-4.4509e-01, -5.4069e-02, -5.4772e-01, -9.1002e-02,  2.4530e-01,\n",
       "           2.9635e-01, -4.3259e-01]],\n",
       "\n",
       "        [[-2.5722e-01, -2.0713e-01, -2.8473e-01, -1.8968e-01,  2.7049e-01,\n",
       "           2.8277e-01, -4.1853e-01]],\n",
       "\n",
       "        [[ 2.3128e-02, -3.0625e-01,  1.3738e-01, -3.0400e-01, -1.2575e-01,\n",
       "           3.4639e-02, -3.9046e-01]],\n",
       "\n",
       "        [[ 2.9769e-01, -3.8736e-01,  3.9362e-01, -3.1438e-01, -1.6354e-01,\n",
       "          -6.1807e-02, -3.9889e-01]],\n",
       "\n",
       "        [[ 3.5258e-01, -3.6032e-01,  4.2549e-01, -3.2476e-01, -6.1945e-03,\n",
       "           6.2259e-02, -4.4101e-01]],\n",
       "\n",
       "        [[ 3.4391e-01, -3.6032e-01,  4.1375e-01, -3.1949e-01, -3.7787e-02,\n",
       "           1.1750e-01, -4.4381e-01]],\n",
       "\n",
       "        [[ 2.9769e-01, -3.3329e-01,  4.1040e-01, -2.5722e-01, -1.6973e-01,\n",
       "           6.2259e-02, -4.6070e-01]],\n",
       "\n",
       "        [[ 4.0173e-01, -7.2091e-02,  5.3602e-01,  3.3696e-02, -2.5150e-01,\n",
       "          -3.4639e-02, -4.6629e-01]],\n",
       "\n",
       "        [[ 6.4738e-01,  2.1627e-01,  6.7838e-01,  2.3631e-01,  2.2011e-01,\n",
       "           1.7229e-01, -4.6912e-01]],\n",
       "\n",
       "        [[ 6.5894e-01,  3.6933e-01,  6.9681e-01,  2.7783e-01,  1.6353e-01,\n",
       "           8.9880e-02, -4.7755e-01]],\n",
       "\n",
       "        [[ 6.6183e-01,  4.1439e-01,  7.0520e-01,  3.7651e-01,  1.8233e-01,\n",
       "           1.1750e-01, -4.8877e-01]],\n",
       "\n",
       "        [[ 5.5780e-01,  2.6133e-01,  6.1809e-01,  2.6745e-01,  1.0696e-01,\n",
       "           4.8223e-02, -4.8314e-01]],\n",
       "\n",
       "        [[ 5.9249e-01,  3.2428e-01,  6.6834e-01,  3.7139e-01,  1.1955e-01,\n",
       "           8.9880e-02, -4.8597e-01]],\n",
       "\n",
       "        [[ 5.6647e-01,  3.6032e-01,  6.3318e-01,  2.8821e-01,  1.4474e-01,\n",
       "           1.4467e-01, -4.8877e-01]],\n",
       "\n",
       "        [[ 6.6183e-01,  4.1439e-01,  7.0685e-01,  3.9215e-01,  2.3271e-01,\n",
       "           1.3108e-01, -4.8877e-01]],\n",
       "\n",
       "        [[ 6.5894e-01,  3.0639e-01,  6.4157e-01,  2.6233e-01,  3.3326e-01,\n",
       "           3.1039e-01, -4.9440e-01]],\n",
       "\n",
       "        [[ 5.6647e-01,  1.3517e-01,  5.4442e-01,  5.9718e-02,  4.1503e-01,\n",
       "           2.2753e-01, -5.0842e-01]],\n",
       "\n",
       "        [[ 5.1996e-02,  2.7034e-02,  1.4573e-01,  8.0477e-02, -1.2575e-01,\n",
       "          -7.0183e-03, -5.1125e-01]],\n",
       "\n",
       "        [[-3.2371e-01, -1.2616e-01, -2.8139e-01, -1.3084e-02, -2.0132e-01,\n",
       "           6.2259e-02, -4.6629e-01]],\n",
       "\n",
       "        [[-4.9135e-01,  8.1103e-02, -4.9582e-01,  1.4275e-01, -1.5713e-01,\n",
       "           6.2259e-02, -3.9889e-01]],\n",
       "\n",
       "        [[-6.0695e-01,  1.2616e-01, -6.0635e-01,  1.0650e-01, -1.3215e-01,\n",
       "           1.0346e-01, -4.0452e-01]],\n",
       "\n",
       "        [[-7.7458e-01, -5.4069e-02, -7.7723e-01,  1.8200e-02, -2.9548e-01,\n",
       "          -7.0183e-03, -4.0168e-01]],\n",
       "\n",
       "        [[-7.4278e-01,  6.3080e-02, -7.1020e-01,  9.0856e-02, -3.1447e-01,\n",
       "          -7.0183e-03, -3.7361e-01]],\n",
       "\n",
       "        [[-6.2140e-01,  4.5057e-02, -5.6784e-01,  1.3237e-01, -3.1447e-01,\n",
       "           7.5843e-02, -3.3148e-01]],\n",
       "\n",
       "        [[-4.3353e-01, -7.2091e-02, -3.9028e-01,  7.0097e-02, -3.4586e-01,\n",
       "           6.2259e-02, -3.6798e-01]],\n",
       "\n",
       "        [[-1.7920e-01, -1.8911e-01, -9.2130e-02, -7.5360e-02, -2.2011e-01,\n",
       "           2.0602e-02, -3.8483e-01]],\n",
       "\n",
       "        [[ 2.0229e-01, -1.7108e-01,  2.5461e-01, -1.3252e-01, -2.5191e-02,\n",
       "           1.5870e-01, -3.7641e-01]],\n",
       "\n",
       "        [[ 3.0926e-01, -2.1614e-01,  3.8024e-01, -1.6892e-01, -5.0382e-02,\n",
       "           1.7229e-01, -3.4550e-01]],\n",
       "\n",
       "        [[ 3.6703e-01, -1.7108e-01,  4.2379e-01, -1.4816e-01,  7.5573e-02,\n",
       "           1.4467e-01, -3.6235e-01]],\n",
       "\n",
       "        [[ 3.1215e-01, -2.1614e-01,  4.0201e-01, -1.4816e-01, -8.7962e-02,\n",
       "           6.2259e-02, -3.8483e-01]],\n",
       "\n",
       "        [[ 3.6992e-01, -1.0814e-01,  4.6565e-01, -7.0243e-02, -1.0056e-01,\n",
       "           8.9880e-02, -3.9609e-01]],\n",
       "\n",
       "        [[ 4.4509e-01,  1.3517e-01,  5.2933e-01,  1.0650e-01, -2.2011e-01,\n",
       "           1.4467e-01, -4.0168e-01]],\n",
       "\n",
       "        [[ 5.6358e-01,  1.8924e-01,  5.6954e-01,  2.5196e-01,  2.9568e-01,\n",
       "           2.2753e-01, -4.1294e-01]],\n",
       "\n",
       "        [[ 6.5316e-01,  3.9637e-01,  6.6669e-01,  3.9215e-01,  2.5150e-01,\n",
       "           2.5515e-01, -4.1294e-01]],\n",
       "\n",
       "        [[ 6.3581e-01,  3.7835e-01,  6.4322e-01,  2.6745e-01,  2.5150e-01,\n",
       "           1.7229e-01, -4.1853e-01]],\n",
       "\n",
       "        [[ 6.3581e-01,  4.1439e-01,  6.4826e-01,  3.6101e-01,  2.8309e-01,\n",
       "           2.9635e-01, -4.1011e-01]],\n",
       "\n",
       "        [[ 6.0117e-01,  2.7935e-01,  6.2813e-01,  2.7783e-01,  2.0132e-01,\n",
       "           2.5515e-01, -4.0731e-01]],\n",
       "\n",
       "        [[ 5.7515e-01,  3.7835e-01,  6.1979e-01,  3.5063e-01,  2.3911e-01,\n",
       "           2.1395e-01, -4.0731e-01]],\n",
       "\n",
       "        [[ 5.2022e-01,  8.1103e-02,  5.6115e-01,  9.6119e-02,  2.0132e-01,\n",
       "           1.9991e-01, -4.3822e-01]],\n",
       "\n",
       "        [[ 3.8438e-01, -2.7034e-02,  4.9417e-01,  3.3696e-02, -1.8233e-01,\n",
       "           1.3108e-01, -4.1294e-01]],\n",
       "\n",
       "        [[ 6.0984e-01,  1.8924e-01,  5.7119e-01,  1.6877e-01,  4.9680e-01,\n",
       "           3.1039e-01, -4.2979e-01]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]], device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.clone(data[-input_window:])\n",
    "input[-output_window:] = 0     \n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:29:25.201868Z",
     "start_time": "2023-03-16T07:29:25.187797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 7])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T07:29:39.741006Z",
     "start_time": "2023-03-16T07:29:39.617678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:14:32.728358Z",
     "start_time": "2023-03-16T06:14:32.704372Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot(eval_model, data_source,epoch,scaler):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            data = data.unsqueeze(1)\n",
    "            target = target.unsqueeze(1)            \n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)\n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1,:].squeeze(1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1,:].squeeze(1).cpu()), 0)\n",
    "#             test_result = torch.cat((test_result, output[-1,:].cpu()), 0) #todo: check this. -> looks good to me\n",
    "#             truth = torch.cat((truth, target[-1,:].cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "    \n",
    "    # 取[:700]的数据进行 时序预测效果的展示\n",
    "    test_result_=scaler.inverse_transform(test_result[:700])\n",
    "    truth_=scaler.inverse_transform(truth)\n",
    "#     print(test_result.shape,truth.shape)\n",
    "    for m in range(data_source.shape[-1]):\n",
    "        test_result = test_result_[:,m]\n",
    "        truth = truth_[:,m]\n",
    "        fig = pyplot.figure(1, figsize=(20, 5))\n",
    "        fig.patch.set_facecolor('xkcd:white')\n",
    "        # 对测试结果 test_result [510:] 之后的数据进行预测展示\n",
    "        pyplot.plot([k + 510                 for k in range(190)],test_result[510:],color=\"red\")\n",
    "        pyplot.title('Prediction uncertainty')\n",
    "        pyplot.plot(truth[:700],color=\"black\")\n",
    "        pyplot.legend([\"prediction\", \"true\"], loc=\"upper left\")\n",
    "        ymin, ymax = pyplot.ylim()\n",
    "        # 画出分界限\n",
    "        pyplot.vlines(510, ymin, ymax, color=\"blue\", linestyles=\"dashed\", linewidth=2)\n",
    "        pyplot.ylim(ymin, ymax)\n",
    "        pyplot.xlabel(\"Periods\")\n",
    "        pyplot.ylabel(\"Y\")\n",
    "        pyplot.show()\n",
    "        pyplot.close()\n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:14:42.271498Z",
     "start_time": "2023-03-16T06:14:33.284465Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_23556\\1687738998.py:11: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model).float())\n"
     ]
    }
   ],
   "source": [
    "# 获取数据和模型\n",
    "# train_data, val_data,scaler = get_data()\n",
    "train_data, valid_data, test_data, scaler = get_data()\n",
    "model = TransAm().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:14:50.173541Z",
     "start_time": "2023-03-16T06:14:50.153793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransAm(\n",
       "  (pos_encoder): PositionalEncoding()\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=7, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=7, bias=True)\n",
       "    (norm1): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=7, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=7, bias=True)\n",
       "        (norm1): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=7, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=7, bias=True)\n",
       "        (norm1): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=7, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=7, bias=True)\n",
       "        (norm1): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=7, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T06:15:07.331603Z",
     "start_time": "2023-03-16T06:14:53.790239Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    75/  377 batches | lr 0.010000 | 34.13 ms | loss 0.09880 | ppl     1.10\n",
      "| epoch   1 |   150/  377 batches | lr 0.010000 | 31.08 ms | loss 0.06616 | ppl     1.07\n",
      "| epoch   1 |   225/  377 batches | lr 0.010000 | 31.24 ms | loss 0.05191 | ppl     1.05\n",
      "| epoch   1 |   300/  377 batches | lr 0.010000 | 31.18 ms | loss 0.08892 | ppl     1.09\n",
      "| epoch   1 |   375/  377 batches | lr 0.010000 | 30.77 ms | loss 0.07875 | ppl     1.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.67s | valid loss 0.07283 | valid ppl     1.08\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [89], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 训练模型 损失\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 验证损失\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "Cell \u001b[1;32mIn [84], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, scheduler, epoch, train_data)\u001b[0m\n\u001b[0;32m      9\u001b[0m data, targets \u001b[38;5;241m=\u001b[39m get_batch(train_data, i,batch_size)\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# calculate_loss_over_all_values 为True时，损失将在所有输出值和目标值上计算\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 为False时，损失仅计算 output 和 targets 的最后 output_window 个值\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calculate_loss_over_all_values:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [79], line 29\u001b[0m, in \u001b[0;36mTransAm.forward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     27\u001b[0m src\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# print('j',src.size(),self.src_mask.size())\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, self.src_mask)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:202\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    199\u001b[0m output \u001b[38;5;241m=\u001b[39m src\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 202\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:345\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m--> 345\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:360\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 360\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\nn\\functional.py:1279\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_data, val_data,scaler = get_data()\n",
    "# train_data, valid_data, test_data, scaler = get_data()\n",
    "# model = TransAm().to(device)\n",
    "\n",
    "# 一些超参数\n",
    "\n",
    "epochs = 100 # The number of epochs\n",
    "lr = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "# 初始化为正无穷大，确保第一次比较时，任何值都比 best_val_loss 更小\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    # 训练模型 损失\n",
    "    train(model, optimizer, criterion, scheduler, epoch, train_data)\n",
    "    \n",
    "    # 验证损失\n",
    "    if(epoch % 10 == 0):\n",
    "        val_loss = plot(model, valid_data, epoch, scaler)\n",
    "        # predict_future(model, val_data,200,epoch,scaler)\n",
    "    else:\n",
    "        val_loss = evaluate(model, valid_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        # 将 best_val_loss 更新为当前的验证损失值\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai3",
   "language": "python",
   "name": "xai3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
