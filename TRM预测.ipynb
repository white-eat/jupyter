{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8624798e",
   "metadata": {
    "direction": "ltr"
   },
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90d633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T12:47:49.843247Z",
     "start_time": "2023-03-08T12:47:46.792393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.optim import SGD\n",
    "import torch.utils.data as Data\n",
    "from torch.nn.utils import weight_norm\n",
    "torch.manual_seed(0)\n",
    "import math\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20310613",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683988cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T12:47:53.729080Z",
     "start_time": "2023-03-08T12:47:53.658270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>5.827</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.462</td>\n",
       "      <td>4.203</td>\n",
       "      <td>1.340</td>\n",
       "      <td>30.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>5.693</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.142</td>\n",
       "      <td>1.371</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 02:00:00</td>\n",
       "      <td>5.157</td>\n",
       "      <td>1.741</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.355</td>\n",
       "      <td>3.777</td>\n",
       "      <td>1.218</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 03:00:00</td>\n",
       "      <td>5.090</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.391</td>\n",
       "      <td>3.807</td>\n",
       "      <td>1.279</td>\n",
       "      <td>25.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 04:00:00</td>\n",
       "      <td>5.358</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.462</td>\n",
       "      <td>3.868</td>\n",
       "      <td>1.279</td>\n",
       "      <td>21.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17415</th>\n",
       "      <td>2018-06-26 15:00:00</td>\n",
       "      <td>-1.674</td>\n",
       "      <td>3.550</td>\n",
       "      <td>-5.615</td>\n",
       "      <td>2.132</td>\n",
       "      <td>3.472</td>\n",
       "      <td>1.523</td>\n",
       "      <td>10.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17416</th>\n",
       "      <td>2018-06-26 16:00:00</td>\n",
       "      <td>-5.492</td>\n",
       "      <td>4.287</td>\n",
       "      <td>-9.132</td>\n",
       "      <td>2.274</td>\n",
       "      <td>3.533</td>\n",
       "      <td>1.675</td>\n",
       "      <td>11.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17417</th>\n",
       "      <td>2018-06-26 17:00:00</td>\n",
       "      <td>2.813</td>\n",
       "      <td>3.818</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>2.097</td>\n",
       "      <td>3.716</td>\n",
       "      <td>1.523</td>\n",
       "      <td>10.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17418</th>\n",
       "      <td>2018-06-26 18:00:00</td>\n",
       "      <td>9.243</td>\n",
       "      <td>3.818</td>\n",
       "      <td>5.472</td>\n",
       "      <td>2.097</td>\n",
       "      <td>3.655</td>\n",
       "      <td>1.432</td>\n",
       "      <td>9.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17419</th>\n",
       "      <td>2018-06-26 19:00:00</td>\n",
       "      <td>10.114</td>\n",
       "      <td>3.550</td>\n",
       "      <td>6.183</td>\n",
       "      <td>1.564</td>\n",
       "      <td>3.716</td>\n",
       "      <td>1.462</td>\n",
       "      <td>9.567000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17420 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date    HUFL   HULL   MUFL   MULL   LUFL   LULL  \\\n",
       "0      2016-07-01 00:00:00   5.827  2.009  1.599  0.462  4.203  1.340   \n",
       "1      2016-07-01 01:00:00   5.693  2.076  1.492  0.426  4.142  1.371   \n",
       "2      2016-07-01 02:00:00   5.157  1.741  1.279  0.355  3.777  1.218   \n",
       "3      2016-07-01 03:00:00   5.090  1.942  1.279  0.391  3.807  1.279   \n",
       "4      2016-07-01 04:00:00   5.358  1.942  1.492  0.462  3.868  1.279   \n",
       "...                    ...     ...    ...    ...    ...    ...    ...   \n",
       "17415  2018-06-26 15:00:00  -1.674  3.550 -5.615  2.132  3.472  1.523   \n",
       "17416  2018-06-26 16:00:00  -5.492  4.287 -9.132  2.274  3.533  1.675   \n",
       "17417  2018-06-26 17:00:00   2.813  3.818 -0.817  2.097  3.716  1.523   \n",
       "17418  2018-06-26 18:00:00   9.243  3.818  5.472  2.097  3.655  1.432   \n",
       "17419  2018-06-26 19:00:00  10.114  3.550  6.183  1.564  3.716  1.462   \n",
       "\n",
       "              OT  \n",
       "0      30.531000  \n",
       "1      27.787001  \n",
       "2      27.787001  \n",
       "3      25.044001  \n",
       "4      21.948000  \n",
       "...          ...  \n",
       "17415  10.904000  \n",
       "17416  11.044000  \n",
       "17417  10.271000  \n",
       "17418   9.778000  \n",
       "17419   9.567000  \n",
       "\n",
       "[17420 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入原数据\n",
    "data = pd.read_csv('./ETTDataset/ETTh1.csv')\n",
    "# c_all=pd.Series(data.columns)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230f9b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T12:47:54.122810Z",
     "start_time": "2023-03-08T12:47:54.067744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 00:00:00</td>\n",
       "      <td>0.615599</td>\n",
       "      <td>0.454943</td>\n",
       "      <td>0.628980</td>\n",
       "      <td>0.467510</td>\n",
       "      <td>0.556576</td>\n",
       "      <td>0.613765</td>\n",
       "      <td>0.691018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 01:00:00</td>\n",
       "      <td>0.612708</td>\n",
       "      <td>0.459449</td>\n",
       "      <td>0.626458</td>\n",
       "      <td>0.464878</td>\n",
       "      <td>0.550279</td>\n",
       "      <td>0.620783</td>\n",
       "      <td>0.636233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 02:00:00</td>\n",
       "      <td>0.601143</td>\n",
       "      <td>0.436920</td>\n",
       "      <td>0.621438</td>\n",
       "      <td>0.459689</td>\n",
       "      <td>0.512595</td>\n",
       "      <td>0.586144</td>\n",
       "      <td>0.636233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 03:00:00</td>\n",
       "      <td>0.599698</td>\n",
       "      <td>0.450437</td>\n",
       "      <td>0.621438</td>\n",
       "      <td>0.462320</td>\n",
       "      <td>0.515693</td>\n",
       "      <td>0.599955</td>\n",
       "      <td>0.581468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 04:00:00</td>\n",
       "      <td>0.605480</td>\n",
       "      <td>0.450437</td>\n",
       "      <td>0.626458</td>\n",
       "      <td>0.467510</td>\n",
       "      <td>0.521990</td>\n",
       "      <td>0.599955</td>\n",
       "      <td>0.519656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17415</th>\n",
       "      <td>2018-06-26 15:00:00</td>\n",
       "      <td>0.453765</td>\n",
       "      <td>0.558574</td>\n",
       "      <td>0.458955</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>0.481107</td>\n",
       "      <td>0.655196</td>\n",
       "      <td>0.299159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17416</th>\n",
       "      <td>2018-06-26 16:00:00</td>\n",
       "      <td>0.371392</td>\n",
       "      <td>0.608137</td>\n",
       "      <td>0.376064</td>\n",
       "      <td>0.599956</td>\n",
       "      <td>0.487404</td>\n",
       "      <td>0.689608</td>\n",
       "      <td>0.301955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17417</th>\n",
       "      <td>2018-06-26 17:00:00</td>\n",
       "      <td>0.550572</td>\n",
       "      <td>0.576597</td>\n",
       "      <td>0.572038</td>\n",
       "      <td>0.587018</td>\n",
       "      <td>0.506298</td>\n",
       "      <td>0.655196</td>\n",
       "      <td>0.286521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17418</th>\n",
       "      <td>2018-06-26 18:00:00</td>\n",
       "      <td>0.689299</td>\n",
       "      <td>0.576597</td>\n",
       "      <td>0.720262</td>\n",
       "      <td>0.587018</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.634594</td>\n",
       "      <td>0.276679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17419</th>\n",
       "      <td>2018-06-26 19:00:00</td>\n",
       "      <td>0.708091</td>\n",
       "      <td>0.558574</td>\n",
       "      <td>0.737019</td>\n",
       "      <td>0.548059</td>\n",
       "      <td>0.506298</td>\n",
       "      <td>0.641386</td>\n",
       "      <td>0.272466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17420 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date      HUFL      HULL      MUFL      MULL      LUFL  \\\n",
       "0      2016-07-01 00:00:00  0.615599  0.454943  0.628980  0.467510  0.556576   \n",
       "1      2016-07-01 01:00:00  0.612708  0.459449  0.626458  0.464878  0.550279   \n",
       "2      2016-07-01 02:00:00  0.601143  0.436920  0.621438  0.459689  0.512595   \n",
       "3      2016-07-01 03:00:00  0.599698  0.450437  0.621438  0.462320  0.515693   \n",
       "4      2016-07-01 04:00:00  0.605480  0.450437  0.626458  0.467510  0.521990   \n",
       "...                    ...       ...       ...       ...       ...       ...   \n",
       "17415  2018-06-26 15:00:00  0.453765  0.558574  0.458955  0.589577  0.481107   \n",
       "17416  2018-06-26 16:00:00  0.371392  0.608137  0.376064  0.599956  0.487404   \n",
       "17417  2018-06-26 17:00:00  0.550572  0.576597  0.572038  0.587018  0.506298   \n",
       "17418  2018-06-26 18:00:00  0.689299  0.576597  0.720262  0.587018  0.500000   \n",
       "17419  2018-06-26 19:00:00  0.708091  0.558574  0.737019  0.548059  0.506298   \n",
       "\n",
       "           LULL        OT  \n",
       "0      0.613765  0.691018  \n",
       "1      0.620783  0.636233  \n",
       "2      0.586144  0.636233  \n",
       "3      0.599955  0.581468  \n",
       "4      0.599955  0.519656  \n",
       "...         ...       ...  \n",
       "17415  0.655196  0.299159  \n",
       "17416  0.689608  0.301955  \n",
       "17417  0.655196  0.286521  \n",
       "17418  0.634594  0.276679  \n",
       "17419  0.641386  0.272466  \n",
       "\n",
       "[17420 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入 归一化数据\n",
    "data_S = pd.read_csv('./ETTDataset/ETTh1_S.csv')\n",
    "c_all=pd.Series(data_S.columns.drop('date'))\n",
    "data_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee87f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a0d205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T12:47:56.779187Z",
     "start_time": "2023-03-08T12:47:56.767221Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\",\"LUFL\",\"LULL\"]\n",
    "input = data_S[features]\n",
    "# 标签为 未归一化数据\n",
    "target = data[\"OT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75cf6a2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T12:47:58.402477Z",
     "start_time": "2023-03-08T12:47:58.394499Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 切分训练集和测试集\n",
    "split = int(input.shape[0] * 0.7)\n",
    "\n",
    "train_input = input[:split]\n",
    "test_input = input[split:]\n",
    "\n",
    "train_target = target[:split]\n",
    "test_target = target[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0febe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T12:47:58.823984Z",
     "start_time": "2023-03-08T12:47:58.817999Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练集和测试集转化为张量\n",
    "\n",
    "train_input_tensor = torch.tensor(train_input.values)\n",
    "test_input_tensor = torch.tensor(test_input.values)\n",
    "\n",
    "train_target_tensor = torch.tensor(train_target.values)\n",
    "test_target_tensor = torch.tensor(test_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e561a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe88aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f1ed9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T12:48:00.658907Z",
     "start_time": "2023-03-08T12:48:00.640958Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将input与target合并\n",
    "o_data = pd.concat([input, target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea963fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T12:48:01.802831Z",
     "start_time": "2023-03-08T12:48:01.793857Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 切分训练集和测试集\n",
    "split = int(o_data.shape[0] * 0.7)\n",
    "train_data = o_data[:split]\n",
    "test_data = o_data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae39325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T12:48:03.760488Z",
     "start_time": "2023-03-08T12:48:03.742509Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据转化为tensor\n",
    "train_data_tensor = torch.tensor(train_data.values)\n",
    "test_data_tensor = torch.tensor(test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab7c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcf670",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        # df.iloc[:, :-1] # 选取除最后一列以外的所有列\n",
    "        train_seq = input_data.iloc[:, :-1][i:i+tw]\n",
    "        # 转化为numpy数据\n",
    "        train_seq = train_seq.values\n",
    "        train_label = input_data.iloc[:,-1][i:i+tw]\n",
    "        train_label = train_label.values\n",
    "        \n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return torch.FloatTensor(inout_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b62cb15f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T07:12:46.991967Z",
     "start_time": "2023-03-08T07:12:44.811144Z"
    }
   },
   "outputs": [],
   "source": [
    "inout_seq = []\n",
    "L = len(input_data)\n",
    "for i in range(L-tw):\n",
    "    # df.iloc[:, :-1] # 选取除最后一列以外的所有列\n",
    "    train_seq = input_data.iloc[:, :-1][i:i+tw]\n",
    "    # 转化为numpy数据\n",
    "    train_seq = train_seq.values\n",
    "    train_label = input_data.iloc[:,-1][i:i+tw]\n",
    "    train_label = train_label.values\n",
    "\n",
    "    inout_seq.append((train_seq ,train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e3003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "105f319e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T07:34:32.894282Z",
     "start_time": "2023-03-07T07:34:32.875334Z"
    }
   },
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__() \n",
    "        #max_len参数指定模型将能够处理的最大序列长度，而d_model参数指定输入嵌入的维数\n",
    "        #pe将为输入序列中的每个位置（最多max_len）提供一行，并为每个嵌入维度提供d_model列。\n",
    "        # 使用全零初始化了pe\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ebd9e06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T07:34:28.781265Z",
     "start_time": "2023-03-07T07:34:28.764275Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 模型架构\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=200,num_layers=1,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        # 初始化方法 __init__ 中，定义了几个成员变量如下：\n",
    "        # model_type 是一个字符串变量，表示这个模型类型是 Transformer\n",
    "        self.model_type = 'Transformer'\n",
    "        # src_mask 是一个存储掩码的变量，用于遮蔽输入中的填充\n",
    "        self.src_mask = None\n",
    "        # pos_encoder 是一个位置编码器，用于将输入的位置信息编码到输入的向量中\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        # encoder_layer 是一个 Transformer 编码器层\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        \n",
    "        # transformer_encoder 则是一个由多个编码器层构成的 Transformer 编码器\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # decoder 是一个线性层，将 Transformer 编码器的输出转换为模型的输出\n",
    "        # 将decoder 直接改用线性层\n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "#         self.decoder = nn.Linear(1,1)\n",
    "        # 初始化模型的权重和偏置\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "        # 对输入进行位置编码\n",
    "        src = self.pos_encoder(src)\n",
    "        # Transformer 编码器进行 编码\n",
    "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "        # 线性层 decoder 将编码器的输出转换为模型的输出\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6663567d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T07:34:24.088250Z",
     "start_time": "2023-03-07T07:34:24.076247Z"
    }
   },
   "outputs": [],
   "source": [
    "# 序列\n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        # 数据和标签\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window] , output_window * [0])\n",
    "        train_label = input_data[i:i+tw]\n",
    "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    # 转化成张量\n",
    "    return torch.FloatTensor(inout_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94395419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b06e9d96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:09:55.589897Z",
     "start_time": "2023-03-07T02:09:55.572276Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "def get_data():\n",
    "#     time        = np.arange(0, 400, 0.1)\n",
    "#     amplitude   = np.sin(time) + np.sin(time*0.05) +np.sin(time*0.12) *np.random.normal(-0.2, 0.2, len(time))\n",
    "    \n",
    "    series = pd.read_csv('./ETTDataset/daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "    \n",
    "    # 归一化\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "#     amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    # 划分训练和测试数据\n",
    "    sampels = 2800\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment.. \n",
    "    # 窗口为100，将数据划分成(带标签的)训练和测试数据，并转化为张量\n",
    "    # 转化训练数据\n",
    "    train_sequence = create_inout_sequences(train_data,input_window)\n",
    "    # 训练数据shape[2695,2,100]\n",
    "    train_sequence = train_sequence[:-output_window] #todo: fix hack?\n",
    "\n",
    "    # 转化测试数据\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1) \n",
    "    test_data = create_inout_sequences(test_data,input_window)\n",
    "    # 训练数据shape[745,2,100]\n",
    "    test_data = test_data[:-output_window] #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device),test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dead2664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T07:38:24.811321Z",
     "start_time": "2023-03-07T07:38:24.802350Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据转化：输入标签\n",
    "def get_batch(source, i,batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]   \n",
    "    # 数据切分成 输入和标签\n",
    "    # 数据维度转化为[input_window,batch_size,1]\n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a57543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14a3f90a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:09:59.330973Z",
     "start_time": "2023-03-07T02:09:59.310022Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train(train_data):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        # 数据切分成 输入数据和标签\n",
    "        data, targets = get_batch(train_data, i,batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)        \n",
    "\n",
    "        # calculate_loss_over_all_values 为True时，损失将在所有输出值和目标值上计算\n",
    "        # 为False时，损失仅计算 output 和 targets 的最后 output_window 个值\n",
    "        if calculate_loss_over_all_values:\n",
    "            loss = criterion(output, targets)\n",
    "        else:\n",
    "            loss = criterion(output[-output_window:], targets[-output_window:])\n",
    "    \n",
    "        loss.backward()\n",
    "        # 执行梯度裁剪的函数,parameters参数是表示神经网络参数梯度的张量列表\n",
    "        # max_norm参数是梯度的最大范数，即当参数的范数大于max_norm时，就会对梯度进行削减\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 计算累积的损失值\n",
    "        total_loss += loss.item()\n",
    "        # 计算打印日志的间隔，设置为训练集大小的5分之一:53\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        # batch 是 log_interval 的倍数且不是第 0 个 batch时，开始打印日志\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # 计算当前平均损失\n",
    "            cur_loss = total_loss / log_interval\n",
    "            # 计算从训练开始到现在的时间\n",
    "            elapsed = time.time() - start_time\n",
    "            # 打印日志，其中包括当前 epoch，batch，学习率，时间，损失值和困惑度\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            # 重置 total_loss，以便下一个 log_interval 计算新的平均损失值\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592e429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45a4562a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:10:02.533275Z",
     "start_time": "2023-03-07T02:10:02.509338Z"
    }
   },
   "outputs": [],
   "source": [
    "# 绘图\n",
    "def plot_and_loss(eval_model, data_source,epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                # 最后 output_window 个数据计算loss\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "            # 记录模型在测试集上的预测结果和真实值\n",
    "            #test_result是一个PyTorch张量，正在与 output[-1].view(-1).cpu() 进行连接\n",
    "            #[-1] 用于访问该张量的最后一个元素，view(-1)用来将 output 的最后一个元素重新形状为一个一维张量。\n",
    "            #它存储了模型在测试集上的所有预测结果，每个预测结果都是一个标量。\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            # truth也是一个PyTorch张量，它存储了测试集中的所有真实标签，每个标签也是一个标量。\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "# 尝试保存结果            \n",
    "    test_result = test_result.cpu().numpy()\n",
    "    test_result = pd.DataFrame(test_result)\n",
    "    test_result.to_csv('graph/test_result.csv',encoding='utf-8',index=False)\n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result,color=\"red\")\n",
    "    pyplot.plot(truth[:500],color=\"blue\")\n",
    "#     pyplot.plot(test_result-truth,color=\"green\")\n",
    "    # 网格线\n",
    "    pyplot.grid(True, which='both')\n",
    "    # 位置为y=0（即x轴）的地方添加一条水平线，线的颜色为黑色\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    # 关闭当前的图表窗口\n",
    "    pyplot.close()\n",
    "    \n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc9c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d224f772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:10:05.997422Z",
     "start_time": "2023-03-07T02:10:05.974476Z"
    }
   },
   "outputs": [],
   "source": [
    "# 预测下一步\n",
    "# predict the next n steps based on the input data \n",
    "\n",
    "# def predict_future(eval_model, data_source,steps):\n",
    "#     eval_model.eval() \n",
    "#     total_loss = 0.\n",
    "#     test_result = torch.Tensor(0)    \n",
    "#     truth = torch.Tensor(0)\n",
    "#     _ , data = get_batch(data_source, 0,1)\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(0, steps,1):\n",
    "#             input = torch.clone(data[-input_window:])\n",
    "#             input[-output_window:] = 0 \n",
    "#             output = eval_model(data[-input_window:])                        \n",
    "#             data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "#     data = data.cpu().view(-1)\n",
    "    \n",
    "def predict_future(eval_model, data_source,steps):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    _ , data = get_batch(data_source, 0,1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps,1):\n",
    "            input = torch.clone(data[-input_window:])\n",
    "            input[-output_window:] = 0     \n",
    "            output = eval_model(data[-input_window:])                        \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "    data = data.cpu().view(-1)\n",
    "\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png'%steps)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e972cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b335a57b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:10:08.325613Z",
     "start_time": "2023-03-07T02:10:08.309656Z"
    }
   },
   "outputs": [],
   "source": [
    "# 评估\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-output_window:], targets[-output_window:]).cpu().item()            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0645b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01071b9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:10:11.397979Z",
     "start_time": "2023-03-07T02:10:11.384979Z"
    }
   },
   "outputs": [],
   "source": [
    "# 超参数\n",
    "input_window = 100\n",
    "output_window = 5\n",
    "batch_size = 10 # batch size\n",
    "\n",
    "# calculate_loss_over_all_values = False\n",
    "calculate_loss_over_all_values = True\n",
    "\n",
    "lr = 0.007\n",
    "\n",
    "# 初始化为正无穷大，确保第一次比较时，任何值都比 best_val_loss 更小\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0139697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:10:12.970507Z",
     "start_time": "2023-03-07T02:10:12.846838Z"
    },
    "direction": "ltr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_5156\\4005502798.py:6: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  series = pd.read_csv('./ETTDataset/daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据和 模型\n",
    "train_data, val_data = get_data()\n",
    "model = TransAm().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abfdb1b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:11:17.241865Z",
     "start_time": "2023-03-07T02:11:17.222915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransAm(\n",
       "  (pos_encoder): PositionalEncoding()\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "    (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf68c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf86c783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:16:45.688579Z",
     "start_time": "2023-03-07T02:11:28.625157Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\xai3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    53/  269 batches | lr 0.007000 | 14.61 ms | loss 3.48887 | ppl    32.75\n",
      "| epoch   1 |   106/  269 batches | lr 0.007000 | 10.45 ms | loss 0.04615 | ppl     1.05\n",
      "| epoch   1 |   159/  269 batches | lr 0.007000 | 10.45 ms | loss 0.02783 | ppl     1.03\n",
      "| epoch   1 |   212/  269 batches | lr 0.007000 | 10.39 ms | loss 0.01752 | ppl     1.02\n",
      "| epoch   1 |   265/  269 batches | lr 0.007000 | 10.32 ms | loss 0.01507 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  3.19s | valid loss 0.01674 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    53/  269 batches | lr 0.006723 | 10.62 ms | loss 0.01732 | ppl     1.02\n",
      "| epoch   2 |   106/  269 batches | lr 0.006723 | 10.40 ms | loss 0.01497 | ppl     1.02\n",
      "| epoch   2 |   159/  269 batches | lr 0.006723 | 10.35 ms | loss 0.01696 | ppl     1.02\n",
      "| epoch   2 |   212/  269 batches | lr 0.006723 | 10.42 ms | loss 0.01158 | ppl     1.01\n",
      "| epoch   2 |   265/  269 batches | lr 0.006723 | 10.35 ms | loss 0.01137 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.97s | valid loss 0.01178 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    53/  269 batches | lr 0.006588 | 10.61 ms | loss 0.01746 | ppl     1.02\n",
      "| epoch   3 |   106/  269 batches | lr 0.006588 | 10.28 ms | loss 0.01598 | ppl     1.02\n",
      "| epoch   3 |   159/  269 batches | lr 0.006588 | 10.30 ms | loss 0.01399 | ppl     1.01\n",
      "| epoch   3 |   212/  269 batches | lr 0.006588 | 10.30 ms | loss 0.01026 | ppl     1.01\n",
      "| epoch   3 |   265/  269 batches | lr 0.006588 | 10.34 ms | loss 0.01037 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.95s | valid loss 0.01173 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    53/  269 batches | lr 0.006457 | 10.90 ms | loss 0.01920 | ppl     1.02\n",
      "| epoch   4 |   106/  269 batches | lr 0.006457 | 10.62 ms | loss 0.01293 | ppl     1.01\n",
      "| epoch   4 |   159/  269 batches | lr 0.006457 | 10.25 ms | loss 0.01090 | ppl     1.01\n",
      "| epoch   4 |   212/  269 batches | lr 0.006457 | 10.46 ms | loss 0.00871 | ppl     1.01\n",
      "| epoch   4 |   265/  269 batches | lr 0.006457 | 10.30 ms | loss 0.01053 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  2.99s | valid loss 0.01099 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    53/  269 batches | lr 0.006327 | 10.57 ms | loss 0.01500 | ppl     1.02\n",
      "| epoch   5 |   106/  269 batches | lr 0.006327 | 10.45 ms | loss 0.01136 | ppl     1.01\n",
      "| epoch   5 |   159/  269 batches | lr 0.006327 | 10.25 ms | loss 0.01011 | ppl     1.01\n",
      "| epoch   5 |   212/  269 batches | lr 0.006327 | 10.30 ms | loss 0.00830 | ppl     1.01\n",
      "| epoch   5 |   265/  269 batches | lr 0.006327 | 10.47 ms | loss 0.01014 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  2.97s | valid loss 0.01474 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    53/  269 batches | lr 0.006201 | 11.51 ms | loss 0.01523 | ppl     1.02\n",
      "| epoch   6 |   106/  269 batches | lr 0.006201 | 11.27 ms | loss 0.01063 | ppl     1.01\n",
      "| epoch   6 |   159/  269 batches | lr 0.006201 | 12.26 ms | loss 0.01101 | ppl     1.01\n",
      "| epoch   6 |   212/  269 batches | lr 0.006201 | 11.94 ms | loss 0.00872 | ppl     1.01\n",
      "| epoch   6 |   265/  269 batches | lr 0.006201 | 11.29 ms | loss 0.00921 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  3.30s | valid loss 0.01118 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    53/  269 batches | lr 0.006077 | 10.61 ms | loss 0.01359 | ppl     1.01\n",
      "| epoch   7 |   106/  269 batches | lr 0.006077 | 10.34 ms | loss 0.00987 | ppl     1.01\n",
      "| epoch   7 |   159/  269 batches | lr 0.006077 | 10.94 ms | loss 0.00991 | ppl     1.01\n",
      "| epoch   7 |   212/  269 batches | lr 0.006077 | 11.64 ms | loss 0.00708 | ppl     1.01\n",
      "| epoch   7 |   265/  269 batches | lr 0.006077 | 11.39 ms | loss 0.00793 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  3.13s | valid loss 0.00897 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    53/  269 batches | lr 0.005955 | 11.09 ms | loss 0.01224 | ppl     1.01\n",
      "| epoch   8 |   106/  269 batches | lr 0.005955 | 11.10 ms | loss 0.00955 | ppl     1.01\n",
      "| epoch   8 |   159/  269 batches | lr 0.005955 | 10.38 ms | loss 0.00877 | ppl     1.01\n",
      "| epoch   8 |   212/  269 batches | lr 0.005955 | 10.49 ms | loss 0.00629 | ppl     1.01\n",
      "| epoch   8 |   265/  269 batches | lr 0.005955 | 10.44 ms | loss 0.00756 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  3.04s | valid loss 0.00910 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    53/  269 batches | lr 0.005836 | 10.88 ms | loss 0.01036 | ppl     1.01\n",
      "| epoch   9 |   106/  269 batches | lr 0.005836 | 10.63 ms | loss 0.00809 | ppl     1.01\n",
      "| epoch   9 |   159/  269 batches | lr 0.005836 | 11.15 ms | loss 0.00820 | ppl     1.01\n",
      "| epoch   9 |   212/  269 batches | lr 0.005836 | 10.68 ms | loss 0.00563 | ppl     1.01\n",
      "| epoch   9 |   265/  269 batches | lr 0.005836 | 10.76 ms | loss 0.00678 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  3.08s | valid loss 0.00717 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    53/  269 batches | lr 0.005720 | 11.39 ms | loss 0.00905 | ppl     1.01\n",
      "| epoch  10 |   106/  269 batches | lr 0.005720 | 10.38 ms | loss 0.00762 | ppl     1.01\n",
      "| epoch  10 |   159/  269 batches | lr 0.005720 | 10.29 ms | loss 0.00699 | ppl     1.01\n",
      "| epoch  10 |   212/  269 batches | lr 0.005720 | 10.27 ms | loss 0.00587 | ppl     1.01\n",
      "| epoch  10 |   265/  269 batches | lr 0.005720 | 10.27 ms | loss 0.00609 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  4.28s | valid loss 0.00822 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    53/  269 batches | lr 0.005605 | 10.64 ms | loss 0.00922 | ppl     1.01\n",
      "| epoch  11 |   106/  269 batches | lr 0.005605 | 10.56 ms | loss 0.00790 | ppl     1.01\n",
      "| epoch  11 |   159/  269 batches | lr 0.005605 | 10.78 ms | loss 0.00746 | ppl     1.01\n",
      "| epoch  11 |   212/  269 batches | lr 0.005605 | 11.33 ms | loss 0.00591 | ppl     1.01\n",
      "| epoch  11 |   265/  269 batches | lr 0.005605 | 10.98 ms | loss 0.00624 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  3.33s | valid loss 0.00673 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    53/  269 batches | lr 0.005493 | 10.60 ms | loss 0.00898 | ppl     1.01\n",
      "| epoch  12 |   106/  269 batches | lr 0.005493 | 10.44 ms | loss 0.00772 | ppl     1.01\n",
      "| epoch  12 |   159/  269 batches | lr 0.005493 | 10.19 ms | loss 0.00660 | ppl     1.01\n",
      "| epoch  12 |   212/  269 batches | lr 0.005493 | 10.40 ms | loss 0.00544 | ppl     1.01\n",
      "| epoch  12 |   265/  269 batches | lr 0.005493 | 10.82 ms | loss 0.00577 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  2.99s | valid loss 0.00591 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  13 |    53/  269 batches | lr 0.005383 | 10.86 ms | loss 0.00855 | ppl     1.01\n",
      "| epoch  13 |   106/  269 batches | lr 0.005383 | 10.23 ms | loss 0.00702 | ppl     1.01\n",
      "| epoch  13 |   159/  269 batches | lr 0.005383 | 10.46 ms | loss 0.00620 | ppl     1.01\n",
      "| epoch  13 |   212/  269 batches | lr 0.005383 | 10.27 ms | loss 0.00503 | ppl     1.01\n",
      "| epoch  13 |   265/  269 batches | lr 0.005383 | 10.75 ms | loss 0.00546 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  3.00s | valid loss 0.00550 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    53/  269 batches | lr 0.005275 | 10.83 ms | loss 0.00776 | ppl     1.01\n",
      "| epoch  14 |   106/  269 batches | lr 0.005275 | 10.45 ms | loss 0.00680 | ppl     1.01\n",
      "| epoch  14 |   159/  269 batches | lr 0.005275 | 10.43 ms | loss 0.00643 | ppl     1.01\n",
      "| epoch  14 |   212/  269 batches | lr 0.005275 | 10.75 ms | loss 0.00508 | ppl     1.01\n",
      "| epoch  14 |   265/  269 batches | lr 0.005275 | 10.56 ms | loss 0.00518 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  3.02s | valid loss 0.00529 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    53/  269 batches | lr 0.005170 | 10.81 ms | loss 0.00720 | ppl     1.01\n",
      "| epoch  15 |   106/  269 batches | lr 0.005170 | 10.72 ms | loss 0.00615 | ppl     1.01\n",
      "| epoch  15 |   159/  269 batches | lr 0.005170 | 10.64 ms | loss 0.00572 | ppl     1.01\n",
      "| epoch  15 |   212/  269 batches | lr 0.005170 | 10.79 ms | loss 0.00500 | ppl     1.01\n",
      "| epoch  15 |   265/  269 batches | lr 0.005170 | 10.69 ms | loss 0.00533 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  3.05s | valid loss 0.00539 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    53/  269 batches | lr 0.005067 | 10.89 ms | loss 0.00707 | ppl     1.01\n",
      "| epoch  16 |   106/  269 batches | lr 0.005067 | 10.72 ms | loss 0.00658 | ppl     1.01\n",
      "| epoch  16 |   159/  269 batches | lr 0.005067 | 10.71 ms | loss 0.00634 | ppl     1.01\n",
      "| epoch  16 |   212/  269 batches | lr 0.005067 | 10.37 ms | loss 0.00538 | ppl     1.01\n",
      "| epoch  16 |   265/  269 batches | lr 0.005067 | 10.42 ms | loss 0.00527 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  3.02s | valid loss 0.00458 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    53/  269 batches | lr 0.004965 | 11.11 ms | loss 0.00663 | ppl     1.01\n",
      "| epoch  17 |   106/  269 batches | lr 0.004965 | 10.68 ms | loss 0.00590 | ppl     1.01\n",
      "| epoch  17 |   159/  269 batches | lr 0.004965 | 10.96 ms | loss 0.00524 | ppl     1.01\n",
      "| epoch  17 |   212/  269 batches | lr 0.004965 | 10.90 ms | loss 0.00473 | ppl     1.00\n",
      "| epoch  17 |   265/  269 batches | lr 0.004965 | 10.50 ms | loss 0.00475 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  3.08s | valid loss 0.00495 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    53/  269 batches | lr 0.004866 | 10.46 ms | loss 0.00639 | ppl     1.01\n",
      "| epoch  18 |   106/  269 batches | lr 0.004866 | 10.38 ms | loss 0.00569 | ppl     1.01\n",
      "| epoch  18 |   159/  269 batches | lr 0.004866 | 10.35 ms | loss 0.00585 | ppl     1.01\n",
      "| epoch  18 |   212/  269 batches | lr 0.004866 | 11.22 ms | loss 0.00477 | ppl     1.00\n",
      "| epoch  18 |   265/  269 batches | lr 0.004866 | 10.84 ms | loss 0.00563 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  3.03s | valid loss 0.00608 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    53/  269 batches | lr 0.004769 | 11.09 ms | loss 0.00708 | ppl     1.01\n",
      "| epoch  19 |   106/  269 batches | lr 0.004769 | 10.73 ms | loss 0.00575 | ppl     1.01\n",
      "| epoch  19 |   159/  269 batches | lr 0.004769 | 10.43 ms | loss 0.00589 | ppl     1.01\n",
      "| epoch  19 |   212/  269 batches | lr 0.004769 | 10.34 ms | loss 0.00493 | ppl     1.00\n",
      "| epoch  19 |   265/  269 batches | lr 0.004769 | 10.32 ms | loss 0.00467 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  3.01s | valid loss 0.00459 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    53/  269 batches | lr 0.004673 | 10.65 ms | loss 0.00625 | ppl     1.01\n",
      "| epoch  20 |   106/  269 batches | lr 0.004673 | 11.01 ms | loss 0.00538 | ppl     1.01\n",
      "| epoch  20 |   159/  269 batches | lr 0.004673 | 10.54 ms | loss 0.00503 | ppl     1.01\n",
      "| epoch  20 |   212/  269 batches | lr 0.004673 | 10.96 ms | loss 0.00439 | ppl     1.00\n",
      "| epoch  20 |   265/  269 batches | lr 0.004673 | 10.78 ms | loss 0.00451 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  4.50s | valid loss 0.00546 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |    53/  269 batches | lr 0.004580 | 10.45 ms | loss 0.00624 | ppl     1.01\n",
      "| epoch  21 |   106/  269 batches | lr 0.004580 | 10.24 ms | loss 0.00550 | ppl     1.01\n",
      "| epoch  21 |   159/  269 batches | lr 0.004580 | 10.35 ms | loss 0.00528 | ppl     1.01\n",
      "| epoch  21 |   212/  269 batches | lr 0.004580 | 10.64 ms | loss 0.00442 | ppl     1.00\n",
      "| epoch  21 |   265/  269 batches | lr 0.004580 | 10.34 ms | loss 0.00445 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  2.96s | valid loss 0.00364 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |    53/  269 batches | lr 0.004488 | 10.46 ms | loss 0.00574 | ppl     1.01\n",
      "| epoch  22 |   106/  269 batches | lr 0.004488 | 10.32 ms | loss 0.00541 | ppl     1.01\n",
      "| epoch  22 |   159/  269 batches | lr 0.004488 | 10.24 ms | loss 0.00508 | ppl     1.01\n",
      "| epoch  22 |   212/  269 batches | lr 0.004488 | 10.26 ms | loss 0.00438 | ppl     1.00\n",
      "| epoch  22 |   265/  269 batches | lr 0.004488 | 10.33 ms | loss 0.00450 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  2.94s | valid loss 0.00453 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |    53/  269 batches | lr 0.004398 | 10.55 ms | loss 0.00573 | ppl     1.01\n",
      "| epoch  23 |   106/  269 batches | lr 0.004398 | 10.53 ms | loss 0.00533 | ppl     1.01\n",
      "| epoch  23 |   159/  269 batches | lr 0.004398 | 10.72 ms | loss 0.00487 | ppl     1.00\n",
      "| epoch  23 |   212/  269 batches | lr 0.004398 | 10.49 ms | loss 0.00426 | ppl     1.00\n",
      "| epoch  23 |   265/  269 batches | lr 0.004398 | 10.33 ms | loss 0.00420 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  2.99s | valid loss 0.00436 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |    53/  269 batches | lr 0.004310 | 10.55 ms | loss 0.00575 | ppl     1.01\n",
      "| epoch  24 |   106/  269 batches | lr 0.004310 | 10.61 ms | loss 0.00529 | ppl     1.01\n",
      "| epoch  24 |   159/  269 batches | lr 0.004310 | 10.53 ms | loss 0.00468 | ppl     1.00\n",
      "| epoch  24 |   212/  269 batches | lr 0.004310 | 10.43 ms | loss 0.00406 | ppl     1.00\n",
      "| epoch  24 |   265/  269 batches | lr 0.004310 | 10.29 ms | loss 0.00426 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  2.98s | valid loss 0.00425 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  25 |    53/  269 batches | lr 0.004224 | 10.77 ms | loss 0.00563 | ppl     1.01\n",
      "| epoch  25 |   106/  269 batches | lr 0.004224 | 10.89 ms | loss 0.00514 | ppl     1.01\n",
      "| epoch  25 |   159/  269 batches | lr 0.004224 | 10.62 ms | loss 0.00459 | ppl     1.00\n",
      "| epoch  25 |   212/  269 batches | lr 0.004224 | 10.43 ms | loss 0.00416 | ppl     1.00\n",
      "| epoch  25 |   265/  269 batches | lr 0.004224 | 10.41 ms | loss 0.00440 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  3.02s | valid loss 0.00465 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |    53/  269 batches | lr 0.004140 | 10.51 ms | loss 0.00548 | ppl     1.01\n",
      "| epoch  26 |   106/  269 batches | lr 0.004140 | 10.34 ms | loss 0.00501 | ppl     1.01\n",
      "| epoch  26 |   159/  269 batches | lr 0.004140 | 10.30 ms | loss 0.00469 | ppl     1.00\n",
      "| epoch  26 |   212/  269 batches | lr 0.004140 | 10.84 ms | loss 0.00402 | ppl     1.00\n",
      "| epoch  26 |   265/  269 batches | lr 0.004140 | 10.74 ms | loss 0.00418 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  3.00s | valid loss 0.00533 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |    53/  269 batches | lr 0.004057 | 10.96 ms | loss 0.00566 | ppl     1.01\n",
      "| epoch  27 |   106/  269 batches | lr 0.004057 | 10.60 ms | loss 0.00510 | ppl     1.01\n",
      "| epoch  27 |   159/  269 batches | lr 0.004057 | 10.87 ms | loss 0.00451 | ppl     1.00\n",
      "| epoch  27 |   212/  269 batches | lr 0.004057 | 11.30 ms | loss 0.00381 | ppl     1.00\n",
      "| epoch  27 |   265/  269 batches | lr 0.004057 | 11.57 ms | loss 0.00412 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  3.15s | valid loss 0.00472 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |    53/  269 batches | lr 0.003976 | 12.39 ms | loss 0.00557 | ppl     1.01\n",
      "| epoch  28 |   106/  269 batches | lr 0.003976 | 11.12 ms | loss 0.00473 | ppl     1.00\n",
      "| epoch  28 |   159/  269 batches | lr 0.003976 | 10.36 ms | loss 0.00452 | ppl     1.00\n",
      "| epoch  28 |   212/  269 batches | lr 0.003976 | 11.23 ms | loss 0.00413 | ppl     1.00\n",
      "| epoch  28 |   265/  269 batches | lr 0.003976 | 11.30 ms | loss 0.00414 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  3.20s | valid loss 0.00474 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |    53/  269 batches | lr 0.003896 | 11.31 ms | loss 0.00537 | ppl     1.01\n",
      "| epoch  29 |   106/  269 batches | lr 0.003896 | 11.09 ms | loss 0.00507 | ppl     1.01\n",
      "| epoch  29 |   159/  269 batches | lr 0.003896 | 10.87 ms | loss 0.00439 | ppl     1.00\n",
      "| epoch  29 |   212/  269 batches | lr 0.003896 | 10.34 ms | loss 0.00397 | ppl     1.00\n",
      "| epoch  29 |   265/  269 batches | lr 0.003896 | 10.82 ms | loss 0.00394 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  3.09s | valid loss 0.00548 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |    53/  269 batches | lr 0.003818 | 10.79 ms | loss 0.00554 | ppl     1.01\n",
      "| epoch  30 |   106/  269 batches | lr 0.003818 | 10.60 ms | loss 0.00452 | ppl     1.00\n",
      "| epoch  30 |   159/  269 batches | lr 0.003818 | 10.63 ms | loss 0.00433 | ppl     1.00\n",
      "| epoch  30 |   212/  269 batches | lr 0.003818 | 10.79 ms | loss 0.00353 | ppl     1.00\n",
      "| epoch  30 |   265/  269 batches | lr 0.003818 | 10.63 ms | loss 0.00387 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  4.38s | valid loss 0.00441 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |    53/  269 batches | lr 0.003742 | 10.60 ms | loss 0.00536 | ppl     1.01\n",
      "| epoch  31 |   106/  269 batches | lr 0.003742 | 10.29 ms | loss 0.00443 | ppl     1.00\n",
      "| epoch  31 |   159/  269 batches | lr 0.003742 | 10.31 ms | loss 0.00438 | ppl     1.00\n",
      "| epoch  31 |   212/  269 batches | lr 0.003742 | 10.44 ms | loss 0.00366 | ppl     1.00\n",
      "| epoch  31 |   265/  269 batches | lr 0.003742 | 10.36 ms | loss 0.00378 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  2.96s | valid loss 0.00480 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |    53/  269 batches | lr 0.003667 | 10.61 ms | loss 0.00508 | ppl     1.01\n",
      "| epoch  32 |   106/  269 batches | lr 0.003667 | 10.29 ms | loss 0.00437 | ppl     1.00\n",
      "| epoch  32 |   159/  269 batches | lr 0.003667 | 10.38 ms | loss 0.00423 | ppl     1.00\n",
      "| epoch  32 |   212/  269 batches | lr 0.003667 | 10.30 ms | loss 0.00341 | ppl     1.00\n",
      "| epoch  32 |   265/  269 batches | lr 0.003667 | 10.38 ms | loss 0.00381 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  2.96s | valid loss 0.00380 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |    53/  269 batches | lr 0.003594 | 10.54 ms | loss 0.00502 | ppl     1.01\n",
      "| epoch  33 |   106/  269 batches | lr 0.003594 | 10.29 ms | loss 0.00443 | ppl     1.00\n",
      "| epoch  33 |   159/  269 batches | lr 0.003594 | 10.39 ms | loss 0.00426 | ppl     1.00\n",
      "| epoch  33 |   212/  269 batches | lr 0.003594 | 10.28 ms | loss 0.00360 | ppl     1.00\n",
      "| epoch  33 |   265/  269 batches | lr 0.003594 | 10.32 ms | loss 0.00382 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  2.95s | valid loss 0.00358 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |    53/  269 batches | lr 0.003522 | 10.47 ms | loss 0.00505 | ppl     1.01\n",
      "| epoch  34 |   106/  269 batches | lr 0.003522 | 10.52 ms | loss 0.00453 | ppl     1.00\n",
      "| epoch  34 |   159/  269 batches | lr 0.003522 | 10.44 ms | loss 0.00438 | ppl     1.00\n",
      "| epoch  34 |   212/  269 batches | lr 0.003522 | 10.27 ms | loss 0.00377 | ppl     1.00\n",
      "| epoch  34 |   265/  269 batches | lr 0.003522 | 10.25 ms | loss 0.00365 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  2.96s | valid loss 0.00402 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |    53/  269 batches | lr 0.003452 | 10.54 ms | loss 0.00486 | ppl     1.00\n",
      "| epoch  35 |   106/  269 batches | lr 0.003452 | 10.16 ms | loss 0.00471 | ppl     1.00\n",
      "| epoch  35 |   159/  269 batches | lr 0.003452 | 10.30 ms | loss 0.00405 | ppl     1.00\n",
      "| epoch  35 |   212/  269 batches | lr 0.003452 | 10.20 ms | loss 0.00347 | ppl     1.00\n",
      "| epoch  35 |   265/  269 batches | lr 0.003452 | 10.30 ms | loss 0.00367 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  2.93s | valid loss 0.00369 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |    53/  269 batches | lr 0.003382 | 10.53 ms | loss 0.00478 | ppl     1.00\n",
      "| epoch  36 |   106/  269 batches | lr 0.003382 | 10.29 ms | loss 0.00458 | ppl     1.00\n",
      "| epoch  36 |   159/  269 batches | lr 0.003382 | 10.22 ms | loss 0.00430 | ppl     1.00\n",
      "| epoch  36 |   212/  269 batches | lr 0.003382 | 10.37 ms | loss 0.00371 | ppl     1.00\n",
      "| epoch  36 |   265/  269 batches | lr 0.003382 | 10.70 ms | loss 0.00370 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  2.97s | valid loss 0.00431 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  37 |    53/  269 batches | lr 0.003315 | 10.87 ms | loss 0.00492 | ppl     1.00\n",
      "| epoch  37 |   106/  269 batches | lr 0.003315 | 10.67 ms | loss 0.00458 | ppl     1.00\n",
      "| epoch  37 |   159/  269 batches | lr 0.003315 | 10.81 ms | loss 0.00420 | ppl     1.00\n",
      "| epoch  37 |   212/  269 batches | lr 0.003315 | 10.97 ms | loss 0.00348 | ppl     1.00\n",
      "| epoch  37 |   265/  269 batches | lr 0.003315 | 10.35 ms | loss 0.00348 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  3.05s | valid loss 0.00425 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |    53/  269 batches | lr 0.003249 | 10.94 ms | loss 0.00470 | ppl     1.00\n",
      "| epoch  38 |   106/  269 batches | lr 0.003249 | 10.91 ms | loss 0.00415 | ppl     1.00\n",
      "| epoch  38 |   159/  269 batches | lr 0.003249 | 11.11 ms | loss 0.00407 | ppl     1.00\n",
      "| epoch  38 |   212/  269 batches | lr 0.003249 | 10.57 ms | loss 0.00327 | ppl     1.00\n",
      "| epoch  38 |   265/  269 batches | lr 0.003249 | 10.65 ms | loss 0.00337 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  3.07s | valid loss 0.00387 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |    53/  269 batches | lr 0.003184 | 10.84 ms | loss 0.00484 | ppl     1.00\n",
      "| epoch  39 |   106/  269 batches | lr 0.003184 | 10.88 ms | loss 0.00442 | ppl     1.00\n",
      "| epoch  39 |   159/  269 batches | lr 0.003184 | 10.60 ms | loss 0.00384 | ppl     1.00\n",
      "| epoch  39 |   212/  269 batches | lr 0.003184 | 10.67 ms | loss 0.00322 | ppl     1.00\n",
      "| epoch  39 |   265/  269 batches | lr 0.003184 | 10.79 ms | loss 0.00344 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  3.07s | valid loss 0.00375 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |    53/  269 batches | lr 0.003120 | 11.05 ms | loss 0.00437 | ppl     1.00\n",
      "| epoch  40 |   106/  269 batches | lr 0.003120 | 10.74 ms | loss 0.00430 | ppl     1.00\n",
      "| epoch  40 |   159/  269 batches | lr 0.003120 | 10.55 ms | loss 0.00388 | ppl     1.00\n",
      "| epoch  40 |   212/  269 batches | lr 0.003120 | 10.30 ms | loss 0.00300 | ppl     1.00\n",
      "| epoch  40 |   265/  269 batches | lr 0.003120 | 10.27 ms | loss 0.00328 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  4.24s | valid loss 0.00352 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |    53/  269 batches | lr 0.003058 | 10.59 ms | loss 0.00415 | ppl     1.00\n",
      "| epoch  41 |   106/  269 batches | lr 0.003058 | 10.30 ms | loss 0.00392 | ppl     1.00\n",
      "| epoch  41 |   159/  269 batches | lr 0.003058 | 10.57 ms | loss 0.00389 | ppl     1.00\n",
      "| epoch  41 |   212/  269 batches | lr 0.003058 | 10.56 ms | loss 0.00312 | ppl     1.00\n",
      "| epoch  41 |   265/  269 batches | lr 0.003058 | 10.48 ms | loss 0.00334 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time:  2.99s | valid loss 0.00440 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |    53/  269 batches | lr 0.002996 | 10.81 ms | loss 0.00442 | ppl     1.00\n",
      "| epoch  42 |   106/  269 batches | lr 0.002996 | 10.66 ms | loss 0.00365 | ppl     1.00\n",
      "| epoch  42 |   159/  269 batches | lr 0.002996 | 10.29 ms | loss 0.00348 | ppl     1.00\n",
      "| epoch  42 |   212/  269 batches | lr 0.002996 | 10.34 ms | loss 0.00311 | ppl     1.00\n",
      "| epoch  42 |   265/  269 batches | lr 0.002996 | 10.78 ms | loss 0.00342 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time:  3.01s | valid loss 0.00411 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |    53/  269 batches | lr 0.002936 | 10.87 ms | loss 0.00407 | ppl     1.00\n",
      "| epoch  43 |   106/  269 batches | lr 0.002936 | 10.53 ms | loss 0.00377 | ppl     1.00\n",
      "| epoch  43 |   159/  269 batches | lr 0.002936 | 10.25 ms | loss 0.00369 | ppl     1.00\n",
      "| epoch  43 |   212/  269 batches | lr 0.002936 | 10.37 ms | loss 0.00305 | ppl     1.00\n",
      "| epoch  43 |   265/  269 batches | lr 0.002936 | 10.31 ms | loss 0.00313 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time:  2.98s | valid loss 0.00401 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |    53/  269 batches | lr 0.002878 | 10.46 ms | loss 0.00409 | ppl     1.00\n",
      "| epoch  44 |   106/  269 batches | lr 0.002878 | 10.23 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch  44 |   159/  269 batches | lr 0.002878 | 10.30 ms | loss 0.00343 | ppl     1.00\n",
      "| epoch  44 |   212/  269 batches | lr 0.002878 | 10.23 ms | loss 0.00290 | ppl     1.00\n",
      "| epoch  44 |   265/  269 batches | lr 0.002878 | 10.43 ms | loss 0.00300 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time:  2.94s | valid loss 0.00402 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |    53/  269 batches | lr 0.002820 | 10.80 ms | loss 0.00411 | ppl     1.00\n",
      "| epoch  45 |   106/  269 batches | lr 0.002820 | 10.29 ms | loss 0.00369 | ppl     1.00\n",
      "| epoch  45 |   159/  269 batches | lr 0.002820 | 10.38 ms | loss 0.00336 | ppl     1.00\n",
      "| epoch  45 |   212/  269 batches | lr 0.002820 | 10.42 ms | loss 0.00277 | ppl     1.00\n",
      "| epoch  45 |   265/  269 batches | lr 0.002820 | 10.42 ms | loss 0.00292 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time:  2.98s | valid loss 0.00328 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |    53/  269 batches | lr 0.002764 | 10.55 ms | loss 0.00403 | ppl     1.00\n",
      "| epoch  46 |   106/  269 batches | lr 0.002764 | 10.45 ms | loss 0.00354 | ppl     1.00\n",
      "| epoch  46 |   159/  269 batches | lr 0.002764 | 10.30 ms | loss 0.00349 | ppl     1.00\n",
      "| epoch  46 |   212/  269 batches | lr 0.002764 | 10.51 ms | loss 0.00286 | ppl     1.00\n",
      "| epoch  46 |   265/  269 batches | lr 0.002764 | 10.29 ms | loss 0.00291 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time:  2.96s | valid loss 0.00325 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |    53/  269 batches | lr 0.002708 | 10.60 ms | loss 0.00378 | ppl     1.00\n",
      "| epoch  47 |   106/  269 batches | lr 0.002708 | 10.55 ms | loss 0.00356 | ppl     1.00\n",
      "| epoch  47 |   159/  269 batches | lr 0.002708 | 10.59 ms | loss 0.00334 | ppl     1.00\n",
      "| epoch  47 |   212/  269 batches | lr 0.002708 | 10.23 ms | loss 0.00290 | ppl     1.00\n",
      "| epoch  47 |   265/  269 batches | lr 0.002708 | 10.30 ms | loss 0.00289 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time:  2.97s | valid loss 0.00377 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |    53/  269 batches | lr 0.002654 | 10.58 ms | loss 0.00383 | ppl     1.00\n",
      "| epoch  48 |   106/  269 batches | lr 0.002654 | 10.42 ms | loss 0.00363 | ppl     1.00\n",
      "| epoch  48 |   159/  269 batches | lr 0.002654 | 10.43 ms | loss 0.00361 | ppl     1.00\n",
      "| epoch  48 |   212/  269 batches | lr 0.002654 | 10.48 ms | loss 0.00281 | ppl     1.00\n",
      "| epoch  48 |   265/  269 batches | lr 0.002654 | 10.72 ms | loss 0.00309 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time:  2.99s | valid loss 0.00331 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  49 |    53/  269 batches | lr 0.002601 | 10.92 ms | loss 0.00388 | ppl     1.00\n",
      "| epoch  49 |   106/  269 batches | lr 0.002601 | 11.17 ms | loss 0.00339 | ppl     1.00\n",
      "| epoch  49 |   159/  269 batches | lr 0.002601 | 11.04 ms | loss 0.00340 | ppl     1.00\n",
      "| epoch  49 |   212/  269 batches | lr 0.002601 | 11.26 ms | loss 0.00321 | ppl     1.00\n",
      "| epoch  49 |   265/  269 batches | lr 0.002601 | 10.70 ms | loss 0.00327 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time:  3.13s | valid loss 0.00380 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |    53/  269 batches | lr 0.002549 | 11.68 ms | loss 0.00397 | ppl     1.00\n",
      "| epoch  50 |   106/  269 batches | lr 0.002549 | 10.89 ms | loss 0.00369 | ppl     1.00\n",
      "| epoch  50 |   159/  269 batches | lr 0.002549 | 10.66 ms | loss 0.00367 | ppl     1.00\n",
      "| epoch  50 |   212/  269 batches | lr 0.002549 | 10.54 ms | loss 0.00308 | ppl     1.00\n",
      "| epoch  50 |   265/  269 batches | lr 0.002549 | 11.50 ms | loss 0.00349 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  4.59s | valid loss 0.00382 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |    53/  269 batches | lr 0.002498 | 10.72 ms | loss 0.00408 | ppl     1.00\n",
      "| epoch  51 |   106/  269 batches | lr 0.002498 | 10.47 ms | loss 0.00383 | ppl     1.00\n",
      "| epoch  51 |   159/  269 batches | lr 0.002498 | 10.89 ms | loss 0.00345 | ppl     1.00\n",
      "| epoch  51 |   212/  269 batches | lr 0.002498 | 10.59 ms | loss 0.00298 | ppl     1.00\n",
      "| epoch  51 |   265/  269 batches | lr 0.002498 | 10.51 ms | loss 0.00303 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time:  3.02s | valid loss 0.00365 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |    53/  269 batches | lr 0.002448 | 10.55 ms | loss 0.00389 | ppl     1.00\n",
      "| epoch  52 |   106/  269 batches | lr 0.002448 | 10.42 ms | loss 0.00339 | ppl     1.00\n",
      "| epoch  52 |   159/  269 batches | lr 0.002448 | 10.33 ms | loss 0.00331 | ppl     1.00\n",
      "| epoch  52 |   212/  269 batches | lr 0.002448 | 10.56 ms | loss 0.00277 | ppl     1.00\n",
      "| epoch  52 |   265/  269 batches | lr 0.002448 | 10.88 ms | loss 0.00300 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time:  3.00s | valid loss 0.00366 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |    53/  269 batches | lr 0.002399 | 11.02 ms | loss 0.00372 | ppl     1.00\n",
      "| epoch  53 |   106/  269 batches | lr 0.002399 | 10.45 ms | loss 0.00349 | ppl     1.00\n",
      "| epoch  53 |   159/  269 batches | lr 0.002399 | 10.52 ms | loss 0.00325 | ppl     1.00\n",
      "| epoch  53 |   212/  269 batches | lr 0.002399 | 10.46 ms | loss 0.00269 | ppl     1.00\n",
      "| epoch  53 |   265/  269 batches | lr 0.002399 | 10.27 ms | loss 0.00292 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time:  3.00s | valid loss 0.00370 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |    53/  269 batches | lr 0.002351 | 10.59 ms | loss 0.00366 | ppl     1.00\n",
      "| epoch  54 |   106/  269 batches | lr 0.002351 | 10.34 ms | loss 0.00327 | ppl     1.00\n",
      "| epoch  54 |   159/  269 batches | lr 0.002351 | 10.46 ms | loss 0.00309 | ppl     1.00\n",
      "| epoch  54 |   212/  269 batches | lr 0.002351 | 10.24 ms | loss 0.00263 | ppl     1.00\n",
      "| epoch  54 |   265/  269 batches | lr 0.002351 | 10.43 ms | loss 0.00278 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time:  2.97s | valid loss 0.00321 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |    53/  269 batches | lr 0.002304 | 10.57 ms | loss 0.00352 | ppl     1.00\n",
      "| epoch  55 |   106/  269 batches | lr 0.002304 | 10.49 ms | loss 0.00325 | ppl     1.00\n",
      "| epoch  55 |   159/  269 batches | lr 0.002304 | 10.71 ms | loss 0.00315 | ppl     1.00\n",
      "| epoch  55 |   212/  269 batches | lr 0.002304 | 10.44 ms | loss 0.00258 | ppl     1.00\n",
      "| epoch  55 |   265/  269 batches | lr 0.002304 | 10.52 ms | loss 0.00271 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time:  3.00s | valid loss 0.00332 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |    53/  269 batches | lr 0.002258 | 10.51 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  56 |   106/  269 batches | lr 0.002258 | 10.32 ms | loss 0.00316 | ppl     1.00\n",
      "| epoch  56 |   159/  269 batches | lr 0.002258 | 10.72 ms | loss 0.00312 | ppl     1.00\n",
      "| epoch  56 |   212/  269 batches | lr 0.002258 | 10.88 ms | loss 0.00253 | ppl     1.00\n",
      "| epoch  56 |   265/  269 batches | lr 0.002258 | 10.41 ms | loss 0.00276 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time:  3.01s | valid loss 0.00355 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |    53/  269 batches | lr 0.002213 | 10.83 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  57 |   106/  269 batches | lr 0.002213 | 10.66 ms | loss 0.00313 | ppl     1.00\n",
      "| epoch  57 |   159/  269 batches | lr 0.002213 | 10.57 ms | loss 0.00311 | ppl     1.00\n",
      "| epoch  57 |   212/  269 batches | lr 0.002213 | 12.61 ms | loss 0.00254 | ppl     1.00\n",
      "| epoch  57 |   265/  269 batches | lr 0.002213 | 11.23 ms | loss 0.00273 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time:  3.19s | valid loss 0.00322 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |    53/  269 batches | lr 0.002169 | 13.57 ms | loss 0.00366 | ppl     1.00\n",
      "| epoch  58 |   106/  269 batches | lr 0.002169 | 11.43 ms | loss 0.00319 | ppl     1.00\n",
      "| epoch  58 |   159/  269 batches | lr 0.002169 | 10.84 ms | loss 0.00317 | ppl     1.00\n",
      "| epoch  58 |   212/  269 batches | lr 0.002169 | 10.85 ms | loss 0.00256 | ppl     1.00\n",
      "| epoch  58 |   265/  269 batches | lr 0.002169 | 11.16 ms | loss 0.00278 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time:  3.27s | valid loss 0.00303 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |    53/  269 batches | lr 0.002125 | 10.55 ms | loss 0.00350 | ppl     1.00\n",
      "| epoch  59 |   106/  269 batches | lr 0.002125 | 10.31 ms | loss 0.00318 | ppl     1.00\n",
      "| epoch  59 |   159/  269 batches | lr 0.002125 | 10.29 ms | loss 0.00311 | ppl     1.00\n",
      "| epoch  59 |   212/  269 batches | lr 0.002125 | 10.46 ms | loss 0.00254 | ppl     1.00\n",
      "| epoch  59 |   265/  269 batches | lr 0.002125 | 10.27 ms | loss 0.00278 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time:  2.95s | valid loss 0.00299 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |    53/  269 batches | lr 0.002083 | 11.11 ms | loss 0.00334 | ppl     1.00\n",
      "| epoch  60 |   106/  269 batches | lr 0.002083 | 10.62 ms | loss 0.00314 | ppl     1.00\n",
      "| epoch  60 |   159/  269 batches | lr 0.002083 | 10.36 ms | loss 0.00315 | ppl     1.00\n",
      "| epoch  60 |   212/  269 batches | lr 0.002083 | 10.36 ms | loss 0.00258 | ppl     1.00\n",
      "| epoch  60 |   265/  269 batches | lr 0.002083 | 10.30 ms | loss 0.00264 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time:  4.24s | valid loss 0.00325 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  61 |    53/  269 batches | lr 0.002041 | 10.53 ms | loss 0.00335 | ppl     1.00\n",
      "| epoch  61 |   106/  269 batches | lr 0.002041 | 10.45 ms | loss 0.00320 | ppl     1.00\n",
      "| epoch  61 |   159/  269 batches | lr 0.002041 | 10.41 ms | loss 0.00305 | ppl     1.00\n",
      "| epoch  61 |   212/  269 batches | lr 0.002041 | 10.88 ms | loss 0.00250 | ppl     1.00\n",
      "| epoch  61 |   265/  269 batches | lr 0.002041 | 10.95 ms | loss 0.00274 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time:  3.03s | valid loss 0.00334 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |    53/  269 batches | lr 0.002000 | 10.73 ms | loss 0.00340 | ppl     1.00\n",
      "| epoch  62 |   106/  269 batches | lr 0.002000 | 10.30 ms | loss 0.00318 | ppl     1.00\n",
      "| epoch  62 |   159/  269 batches | lr 0.002000 | 10.24 ms | loss 0.00314 | ppl     1.00\n",
      "| epoch  62 |   212/  269 batches | lr 0.002000 | 10.33 ms | loss 0.00263 | ppl     1.00\n",
      "| epoch  62 |   265/  269 batches | lr 0.002000 | 10.23 ms | loss 0.00299 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time:  2.95s | valid loss 0.00340 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |    53/  269 batches | lr 0.001960 | 10.39 ms | loss 0.00352 | ppl     1.00\n",
      "| epoch  63 |   106/  269 batches | lr 0.001960 | 10.32 ms | loss 0.00332 | ppl     1.00\n",
      "| epoch  63 |   159/  269 batches | lr 0.001960 | 10.31 ms | loss 0.00311 | ppl     1.00\n",
      "| epoch  63 |   212/  269 batches | lr 0.001960 | 10.47 ms | loss 0.00251 | ppl     1.00\n",
      "| epoch  63 |   265/  269 batches | lr 0.001960 | 10.68 ms | loss 0.00278 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time:  2.97s | valid loss 0.00310 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |    53/  269 batches | lr 0.001921 | 10.94 ms | loss 0.00356 | ppl     1.00\n",
      "| epoch  64 |   106/  269 batches | lr 0.001921 | 10.37 ms | loss 0.00308 | ppl     1.00\n",
      "| epoch  64 |   159/  269 batches | lr 0.001921 | 10.39 ms | loss 0.00306 | ppl     1.00\n",
      "| epoch  64 |   212/  269 batches | lr 0.001921 | 10.47 ms | loss 0.00252 | ppl     1.00\n",
      "| epoch  64 |   265/  269 batches | lr 0.001921 | 10.42 ms | loss 0.00281 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time:  2.99s | valid loss 0.00317 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |    53/  269 batches | lr 0.001883 | 11.04 ms | loss 0.00334 | ppl     1.00\n",
      "| epoch  65 |   106/  269 batches | lr 0.001883 | 11.04 ms | loss 0.00313 | ppl     1.00\n",
      "| epoch  65 |   159/  269 batches | lr 0.001883 | 10.54 ms | loss 0.00307 | ppl     1.00\n",
      "| epoch  65 |   212/  269 batches | lr 0.001883 | 10.67 ms | loss 0.00245 | ppl     1.00\n",
      "| epoch  65 |   265/  269 batches | lr 0.001883 | 10.57 ms | loss 0.00265 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time:  3.06s | valid loss 0.00332 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |    53/  269 batches | lr 0.001845 | 11.37 ms | loss 0.00347 | ppl     1.00\n",
      "| epoch  66 |   106/  269 batches | lr 0.001845 | 10.75 ms | loss 0.00306 | ppl     1.00\n",
      "| epoch  66 |   159/  269 batches | lr 0.001845 | 10.91 ms | loss 0.00292 | ppl     1.00\n",
      "| epoch  66 |   212/  269 batches | lr 0.001845 | 10.49 ms | loss 0.00246 | ppl     1.00\n",
      "| epoch  66 |   265/  269 batches | lr 0.001845 | 10.36 ms | loss 0.00275 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time:  3.06s | valid loss 0.00336 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |    53/  269 batches | lr 0.001808 | 11.08 ms | loss 0.00335 | ppl     1.00\n",
      "| epoch  67 |   106/  269 batches | lr 0.001808 | 11.01 ms | loss 0.00305 | ppl     1.00\n",
      "| epoch  67 |   159/  269 batches | lr 0.001808 | 11.10 ms | loss 0.00296 | ppl     1.00\n",
      "| epoch  67 |   212/  269 batches | lr 0.001808 | 10.72 ms | loss 0.00243 | ppl     1.00\n",
      "| epoch  67 |   265/  269 batches | lr 0.001808 | 10.69 ms | loss 0.00266 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time:  3.11s | valid loss 0.00360 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |    53/  269 batches | lr 0.001772 | 11.08 ms | loss 0.00336 | ppl     1.00\n",
      "| epoch  68 |   106/  269 batches | lr 0.001772 | 10.53 ms | loss 0.00303 | ppl     1.00\n",
      "| epoch  68 |   159/  269 batches | lr 0.001772 | 10.95 ms | loss 0.00298 | ppl     1.00\n",
      "| epoch  68 |   212/  269 batches | lr 0.001772 | 11.00 ms | loss 0.00249 | ppl     1.00\n",
      "| epoch  68 |   265/  269 batches | lr 0.001772 | 10.43 ms | loss 0.00263 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time:  3.07s | valid loss 0.00334 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |    53/  269 batches | lr 0.001737 | 10.70 ms | loss 0.00338 | ppl     1.00\n",
      "| epoch  69 |   106/  269 batches | lr 0.001737 | 10.51 ms | loss 0.00301 | ppl     1.00\n",
      "| epoch  69 |   159/  269 batches | lr 0.001737 | 11.12 ms | loss 0.00290 | ppl     1.00\n",
      "| epoch  69 |   212/  269 batches | lr 0.001737 | 11.04 ms | loss 0.00243 | ppl     1.00\n",
      "| epoch  69 |   265/  269 batches | lr 0.001737 | 10.81 ms | loss 0.00266 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time:  3.08s | valid loss 0.00314 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |    53/  269 batches | lr 0.001702 | 10.81 ms | loss 0.00329 | ppl     1.00\n",
      "| epoch  70 |   106/  269 batches | lr 0.001702 | 10.57 ms | loss 0.00290 | ppl     1.00\n",
      "| epoch  70 |   159/  269 batches | lr 0.001702 | 10.56 ms | loss 0.00288 | ppl     1.00\n",
      "| epoch  70 |   212/  269 batches | lr 0.001702 | 10.39 ms | loss 0.00239 | ppl     1.00\n",
      "| epoch  70 |   265/  269 batches | lr 0.001702 | 10.41 ms | loss 0.00265 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time:  4.30s | valid loss 0.00334 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 |    53/  269 batches | lr 0.001668 | 10.60 ms | loss 0.00330 | ppl     1.00\n",
      "| epoch  71 |   106/  269 batches | lr 0.001668 | 10.44 ms | loss 0.00291 | ppl     1.00\n",
      "| epoch  71 |   159/  269 batches | lr 0.001668 | 10.66 ms | loss 0.00282 | ppl     1.00\n",
      "| epoch  71 |   212/  269 batches | lr 0.001668 | 10.88 ms | loss 0.00242 | ppl     1.00\n",
      "| epoch  71 |   265/  269 batches | lr 0.001668 | 11.08 ms | loss 0.00265 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time:  3.06s | valid loss 0.00321 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 |    53/  269 batches | lr 0.001634 | 11.14 ms | loss 0.00329 | ppl     1.00\n",
      "| epoch  72 |   106/  269 batches | lr 0.001634 | 11.32 ms | loss 0.00294 | ppl     1.00\n",
      "| epoch  72 |   159/  269 batches | lr 0.001634 | 10.73 ms | loss 0.00286 | ppl     1.00\n",
      "| epoch  72 |   212/  269 batches | lr 0.001634 | 11.18 ms | loss 0.00239 | ppl     1.00\n",
      "| epoch  72 |   265/  269 batches | lr 0.001634 | 10.93 ms | loss 0.00259 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time:  3.14s | valid loss 0.00344 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  73 |    53/  269 batches | lr 0.001602 | 11.12 ms | loss 0.00329 | ppl     1.00\n",
      "| epoch  73 |   106/  269 batches | lr 0.001602 | 10.94 ms | loss 0.00286 | ppl     1.00\n",
      "| epoch  73 |   159/  269 batches | lr 0.001602 | 10.77 ms | loss 0.00288 | ppl     1.00\n",
      "| epoch  73 |   212/  269 batches | lr 0.001602 | 10.79 ms | loss 0.00250 | ppl     1.00\n",
      "| epoch  73 |   265/  269 batches | lr 0.001602 | 10.39 ms | loss 0.00272 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time:  3.07s | valid loss 0.00354 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |    53/  269 batches | lr 0.001570 | 11.21 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  74 |   106/  269 batches | lr 0.001570 | 10.58 ms | loss 0.00290 | ppl     1.00\n",
      "| epoch  74 |   159/  269 batches | lr 0.001570 | 10.50 ms | loss 0.00279 | ppl     1.00\n",
      "| epoch  74 |   212/  269 batches | lr 0.001570 | 10.82 ms | loss 0.00249 | ppl     1.00\n",
      "| epoch  74 |   265/  269 batches | lr 0.001570 | 10.81 ms | loss 0.00260 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time:  3.07s | valid loss 0.00326 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 |    53/  269 batches | lr 0.001538 | 10.59 ms | loss 0.00322 | ppl     1.00\n",
      "| epoch  75 |   106/  269 batches | lr 0.001538 | 10.43 ms | loss 0.00286 | ppl     1.00\n",
      "| epoch  75 |   159/  269 batches | lr 0.001538 | 10.37 ms | loss 0.00276 | ppl     1.00\n",
      "| epoch  75 |   212/  269 batches | lr 0.001538 | 10.39 ms | loss 0.00243 | ppl     1.00\n",
      "| epoch  75 |   265/  269 batches | lr 0.001538 | 10.36 ms | loss 0.00301 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time:  2.97s | valid loss 0.00361 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 |    53/  269 batches | lr 0.001508 | 11.02 ms | loss 0.00334 | ppl     1.00\n",
      "| epoch  76 |   106/  269 batches | lr 0.001508 | 11.26 ms | loss 0.00290 | ppl     1.00\n",
      "| epoch  76 |   159/  269 batches | lr 0.001508 | 10.56 ms | loss 0.00287 | ppl     1.00\n",
      "| epoch  76 |   212/  269 batches | lr 0.001508 | 10.50 ms | loss 0.00244 | ppl     1.00\n",
      "| epoch  76 |   265/  269 batches | lr 0.001508 | 10.42 ms | loss 0.00257 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time:  3.06s | valid loss 0.00379 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |    53/  269 batches | lr 0.001477 | 10.61 ms | loss 0.00329 | ppl     1.00\n",
      "| epoch  77 |   106/  269 batches | lr 0.001477 | 10.37 ms | loss 0.00278 | ppl     1.00\n",
      "| epoch  77 |   159/  269 batches | lr 0.001477 | 10.61 ms | loss 0.00276 | ppl     1.00\n",
      "| epoch  77 |   212/  269 batches | lr 0.001477 | 10.94 ms | loss 0.00242 | ppl     1.00\n",
      "| epoch  77 |   265/  269 batches | lr 0.001477 | 10.55 ms | loss 0.00254 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time:  3.02s | valid loss 0.00362 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |    53/  269 batches | lr 0.001448 | 10.65 ms | loss 0.00320 | ppl     1.00\n",
      "| epoch  78 |   106/  269 batches | lr 0.001448 | 10.84 ms | loss 0.00279 | ppl     1.00\n",
      "| epoch  78 |   159/  269 batches | lr 0.001448 | 11.11 ms | loss 0.00278 | ppl     1.00\n",
      "| epoch  78 |   212/  269 batches | lr 0.001448 | 10.93 ms | loss 0.00242 | ppl     1.00\n",
      "| epoch  78 |   265/  269 batches | lr 0.001448 | 10.64 ms | loss 0.00252 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time:  3.09s | valid loss 0.00346 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 |    53/  269 batches | lr 0.001419 | 11.05 ms | loss 0.00319 | ppl     1.00\n",
      "| epoch  79 |   106/  269 batches | lr 0.001419 | 11.14 ms | loss 0.00275 | ppl     1.00\n",
      "| epoch  79 |   159/  269 batches | lr 0.001419 | 10.64 ms | loss 0.00273 | ppl     1.00\n",
      "| epoch  79 |   212/  269 batches | lr 0.001419 | 10.75 ms | loss 0.00242 | ppl     1.00\n",
      "| epoch  79 |   265/  269 batches | lr 0.001419 | 10.83 ms | loss 0.00251 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time:  3.09s | valid loss 0.00344 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |    53/  269 batches | lr 0.001391 | 10.85 ms | loss 0.00315 | ppl     1.00\n",
      "| epoch  80 |   106/  269 batches | lr 0.001391 | 10.40 ms | loss 0.00277 | ppl     1.00\n",
      "| epoch  80 |   159/  269 batches | lr 0.001391 | 10.37 ms | loss 0.00274 | ppl     1.00\n",
      "| epoch  80 |   212/  269 batches | lr 0.001391 | 10.68 ms | loss 0.00241 | ppl     1.00\n",
      "| epoch  80 |   265/  269 batches | lr 0.001391 | 10.95 ms | loss 0.00252 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time:  4.47s | valid loss 0.00340 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 |    53/  269 batches | lr 0.001363 | 10.69 ms | loss 0.00316 | ppl     1.00\n",
      "| epoch  81 |   106/  269 batches | lr 0.001363 | 10.59 ms | loss 0.00280 | ppl     1.00\n",
      "| epoch  81 |   159/  269 batches | lr 0.001363 | 10.64 ms | loss 0.00280 | ppl     1.00\n",
      "| epoch  81 |   212/  269 batches | lr 0.001363 | 10.72 ms | loss 0.00236 | ppl     1.00\n",
      "| epoch  81 |   265/  269 batches | lr 0.001363 | 11.00 ms | loss 0.00248 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time:  3.05s | valid loss 0.00347 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |    53/  269 batches | lr 0.001335 | 10.60 ms | loss 0.00314 | ppl     1.00\n",
      "| epoch  82 |   106/  269 batches | lr 0.001335 | 10.70 ms | loss 0.00277 | ppl     1.00\n",
      "| epoch  82 |   159/  269 batches | lr 0.001335 | 10.64 ms | loss 0.00274 | ppl     1.00\n",
      "| epoch  82 |   212/  269 batches | lr 0.001335 | 10.40 ms | loss 0.00239 | ppl     1.00\n",
      "| epoch  82 |   265/  269 batches | lr 0.001335 | 10.29 ms | loss 0.00253 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time:  2.99s | valid loss 0.00319 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |    53/  269 batches | lr 0.001309 | 10.90 ms | loss 0.00308 | ppl     1.00\n",
      "| epoch  83 |   106/  269 batches | lr 0.001309 | 10.57 ms | loss 0.00274 | ppl     1.00\n",
      "| epoch  83 |   159/  269 batches | lr 0.001309 | 10.43 ms | loss 0.00284 | ppl     1.00\n",
      "| epoch  83 |   212/  269 batches | lr 0.001309 | 10.63 ms | loss 0.00236 | ppl     1.00\n",
      "| epoch  83 |   265/  269 batches | lr 0.001309 | 10.34 ms | loss 0.00250 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time:  3.00s | valid loss 0.00324 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |    53/  269 batches | lr 0.001283 | 10.74 ms | loss 0.00315 | ppl     1.00\n",
      "| epoch  84 |   106/  269 batches | lr 0.001283 | 10.30 ms | loss 0.00276 | ppl     1.00\n",
      "| epoch  84 |   159/  269 batches | lr 0.001283 | 10.59 ms | loss 0.00277 | ppl     1.00\n",
      "| epoch  84 |   212/  269 batches | lr 0.001283 | 10.57 ms | loss 0.00236 | ppl     1.00\n",
      "| epoch  84 |   265/  269 batches | lr 0.001283 | 10.53 ms | loss 0.00253 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time:  3.00s | valid loss 0.00326 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  85 |    53/  269 batches | lr 0.001257 | 10.54 ms | loss 0.00314 | ppl     1.00\n",
      "| epoch  85 |   106/  269 batches | lr 0.001257 | 10.49 ms | loss 0.00277 | ppl     1.00\n",
      "| epoch  85 |   159/  269 batches | lr 0.001257 | 10.58 ms | loss 0.00274 | ppl     1.00\n",
      "| epoch  85 |   212/  269 batches | lr 0.001257 | 10.57 ms | loss 0.00236 | ppl     1.00\n",
      "| epoch  85 |   265/  269 batches | lr 0.001257 | 10.59 ms | loss 0.00253 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time:  3.00s | valid loss 0.00328 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 |    53/  269 batches | lr 0.001232 | 10.82 ms | loss 0.00315 | ppl     1.00\n",
      "| epoch  86 |   106/  269 batches | lr 0.001232 | 10.37 ms | loss 0.00283 | ppl     1.00\n",
      "| epoch  86 |   159/  269 batches | lr 0.001232 | 10.40 ms | loss 0.00272 | ppl     1.00\n",
      "| epoch  86 |   212/  269 batches | lr 0.001232 | 10.35 ms | loss 0.00231 | ppl     1.00\n",
      "| epoch  86 |   265/  269 batches | lr 0.001232 | 10.33 ms | loss 0.00251 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time:  2.97s | valid loss 0.00339 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |    53/  269 batches | lr 0.001207 | 10.73 ms | loss 0.00313 | ppl     1.00\n",
      "| epoch  87 |   106/  269 batches | lr 0.001207 | 10.55 ms | loss 0.00270 | ppl     1.00\n",
      "| epoch  87 |   159/  269 batches | lr 0.001207 | 10.81 ms | loss 0.00270 | ppl     1.00\n",
      "| epoch  87 |   212/  269 batches | lr 0.001207 | 10.42 ms | loss 0.00233 | ppl     1.00\n",
      "| epoch  87 |   265/  269 batches | lr 0.001207 | 10.34 ms | loss 0.00251 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time:  3.00s | valid loss 0.00329 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 |    53/  269 batches | lr 0.001183 | 10.53 ms | loss 0.00314 | ppl     1.00\n",
      "| epoch  88 |   106/  269 batches | lr 0.001183 | 10.36 ms | loss 0.00271 | ppl     1.00\n",
      "| epoch  88 |   159/  269 batches | lr 0.001183 | 10.34 ms | loss 0.00266 | ppl     1.00\n",
      "| epoch  88 |   212/  269 batches | lr 0.001183 | 10.36 ms | loss 0.00231 | ppl     1.00\n",
      "| epoch  88 |   265/  269 batches | lr 0.001183 | 10.66 ms | loss 0.00247 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time:  2.98s | valid loss 0.00313 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |    53/  269 batches | lr 0.001159 | 11.11 ms | loss 0.00306 | ppl     1.00\n",
      "| epoch  89 |   106/  269 batches | lr 0.001159 | 10.73 ms | loss 0.00259 | ppl     1.00\n",
      "| epoch  89 |   159/  269 batches | lr 0.001159 | 10.36 ms | loss 0.00263 | ppl     1.00\n",
      "| epoch  89 |   212/  269 batches | lr 0.001159 | 10.59 ms | loss 0.00225 | ppl     1.00\n",
      "| epoch  89 |   265/  269 batches | lr 0.001159 | 12.07 ms | loss 0.00243 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time:  3.14s | valid loss 0.00298 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |    53/  269 batches | lr 0.001136 | 11.44 ms | loss 0.00306 | ppl     1.00\n",
      "| epoch  90 |   106/  269 batches | lr 0.001136 | 10.64 ms | loss 0.00259 | ppl     1.00\n",
      "| epoch  90 |   159/  269 batches | lr 0.001136 | 10.34 ms | loss 0.00262 | ppl     1.00\n",
      "| epoch  90 |   212/  269 batches | lr 0.001136 | 10.32 ms | loss 0.00225 | ppl     1.00\n",
      "| epoch  90 |   265/  269 batches | lr 0.001136 | 10.37 ms | loss 0.00242 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time:  4.46s | valid loss 0.00294 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |    53/  269 batches | lr 0.001113 | 11.51 ms | loss 0.00300 | ppl     1.00\n",
      "| epoch  91 |   106/  269 batches | lr 0.001113 | 10.51 ms | loss 0.00258 | ppl     1.00\n",
      "| epoch  91 |   159/  269 batches | lr 0.001113 | 10.45 ms | loss 0.00260 | ppl     1.00\n",
      "| epoch  91 |   212/  269 batches | lr 0.001113 | 10.42 ms | loss 0.00224 | ppl     1.00\n",
      "| epoch  91 |   265/  269 batches | lr 0.001113 | 10.62 ms | loss 0.00243 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time:  3.06s | valid loss 0.00324 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |    53/  269 batches | lr 0.001091 | 12.60 ms | loss 0.00296 | ppl     1.00\n",
      "| epoch  92 |   106/  269 batches | lr 0.001091 | 10.81 ms | loss 0.00263 | ppl     1.00\n",
      "| epoch  92 |   159/  269 batches | lr 0.001091 | 10.34 ms | loss 0.00261 | ppl     1.00\n",
      "| epoch  92 |   212/  269 batches | lr 0.001091 | 10.31 ms | loss 0.00221 | ppl     1.00\n",
      "| epoch  92 |   265/  269 batches | lr 0.001091 | 10.26 ms | loss 0.00248 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time:  3.08s | valid loss 0.00301 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 |    53/  269 batches | lr 0.001069 | 10.47 ms | loss 0.00304 | ppl     1.00\n",
      "| epoch  93 |   106/  269 batches | lr 0.001069 | 10.28 ms | loss 0.00261 | ppl     1.00\n",
      "| epoch  93 |   159/  269 batches | lr 0.001069 | 10.26 ms | loss 0.00263 | ppl     1.00\n",
      "| epoch  93 |   212/  269 batches | lr 0.001069 | 11.61 ms | loss 0.00224 | ppl     1.00\n",
      "| epoch  93 |   265/  269 batches | lr 0.001069 | 11.22 ms | loss 0.00239 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time:  3.07s | valid loss 0.00299 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |    53/  269 batches | lr 0.001048 | 10.82 ms | loss 0.00298 | ppl     1.00\n",
      "| epoch  94 |   106/  269 batches | lr 0.001048 | 10.61 ms | loss 0.00260 | ppl     1.00\n",
      "| epoch  94 |   159/  269 batches | lr 0.001048 | 10.71 ms | loss 0.00255 | ppl     1.00\n",
      "| epoch  94 |   212/  269 batches | lr 0.001048 | 10.49 ms | loss 0.00225 | ppl     1.00\n",
      "| epoch  94 |   265/  269 batches | lr 0.001048 | 10.34 ms | loss 0.00243 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time:  3.01s | valid loss 0.00328 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 |    53/  269 batches | lr 0.001027 | 10.79 ms | loss 0.00304 | ppl     1.00\n",
      "| epoch  95 |   106/  269 batches | lr 0.001027 | 10.69 ms | loss 0.00261 | ppl     1.00\n",
      "| epoch  95 |   159/  269 batches | lr 0.001027 | 10.66 ms | loss 0.00255 | ppl     1.00\n",
      "| epoch  95 |   212/  269 batches | lr 0.001027 | 11.14 ms | loss 0.00226 | ppl     1.00\n",
      "| epoch  95 |   265/  269 batches | lr 0.001027 | 11.18 ms | loss 0.00237 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time:  3.10s | valid loss 0.00291 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |    53/  269 batches | lr 0.001006 | 11.11 ms | loss 0.00291 | ppl     1.00\n",
      "| epoch  96 |   106/  269 batches | lr 0.001006 | 10.92 ms | loss 0.00256 | ppl     1.00\n",
      "| epoch  96 |   159/  269 batches | lr 0.001006 | 10.84 ms | loss 0.00253 | ppl     1.00\n",
      "| epoch  96 |   212/  269 batches | lr 0.001006 | 10.84 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch  96 |   265/  269 batches | lr 0.001006 | 10.80 ms | loss 0.00236 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time:  3.09s | valid loss 0.00276 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  97 |    53/  269 batches | lr 0.000986 | 10.62 ms | loss 0.00295 | ppl     1.00\n",
      "| epoch  97 |   106/  269 batches | lr 0.000986 | 10.44 ms | loss 0.00252 | ppl     1.00\n",
      "| epoch  97 |   159/  269 batches | lr 0.000986 | 10.51 ms | loss 0.00251 | ppl     1.00\n",
      "| epoch  97 |   212/  269 batches | lr 0.000986 | 10.81 ms | loss 0.00215 | ppl     1.00\n",
      "| epoch  97 |   265/  269 batches | lr 0.000986 | 10.62 ms | loss 0.00233 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time:  3.02s | valid loss 0.00288 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 |    53/  269 batches | lr 0.000967 | 10.90 ms | loss 0.00286 | ppl     1.00\n",
      "| epoch  98 |   106/  269 batches | lr 0.000967 | 10.48 ms | loss 0.00249 | ppl     1.00\n",
      "| epoch  98 |   159/  269 batches | lr 0.000967 | 10.32 ms | loss 0.00252 | ppl     1.00\n",
      "| epoch  98 |   212/  269 batches | lr 0.000967 | 10.44 ms | loss 0.00218 | ppl     1.00\n",
      "| epoch  98 |   265/  269 batches | lr 0.000967 | 10.36 ms | loss 0.00236 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time:  2.99s | valid loss 0.00277 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |    53/  269 batches | lr 0.000947 | 11.04 ms | loss 0.00288 | ppl     1.00\n",
      "| epoch  99 |   106/  269 batches | lr 0.000947 | 10.76 ms | loss 0.00252 | ppl     1.00\n",
      "| epoch  99 |   159/  269 batches | lr 0.000947 | 10.49 ms | loss 0.00253 | ppl     1.00\n",
      "| epoch  99 |   212/  269 batches | lr 0.000947 | 10.61 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch  99 |   265/  269 batches | lr 0.000947 | 10.57 ms | loss 0.00232 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time:  3.04s | valid loss 0.00279 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |    53/  269 batches | lr 0.000928 | 11.11 ms | loss 0.00291 | ppl     1.00\n",
      "| epoch 100 |   106/  269 batches | lr 0.000928 | 10.83 ms | loss 0.00253 | ppl     1.00\n",
      "| epoch 100 |   159/  269 batches | lr 0.000928 | 11.39 ms | loss 0.00251 | ppl     1.00\n",
      "| epoch 100 |   212/  269 batches | lr 0.000928 | 10.56 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 100 |   265/  269 batches | lr 0.000928 | 10.48 ms | loss 0.00229 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time:  4.38s | valid loss 0.00285 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# 学习率调度器\n",
    "# 每一个epoch都会将学习率乘以0.98,\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data)\n",
    "    \n",
    "    \n",
    "    if(epoch % 10 == 0):\n",
    "        # 每10个epoch开始绘图\n",
    "        val_loss = plot_and_loss(model, val_data,epoch)\n",
    "        # 200数值代表预测未来的步数\n",
    "        predict_future(model, val_data,200)\n",
    "    else:\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        # 将 best_val_loss 更新为当前的验证损失值\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdf71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22ec111e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:17:03.809085Z",
     "start_time": "2023-03-07T02:17:03.790137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002758366634581713"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "541e7f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T02:20:47.884384Z",
     "start_time": "2023-03-07T02:20:47.867430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransAm(\n",
       "  (pos_encoder): PositionalEncoding()\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "    (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "074fcf97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T03:01:08.488032Z",
     "start_time": "2023-03-07T03:01:08.450829Z"
    }
   },
   "outputs": [],
   "source": [
    "# 尝试保存模型 参数\n",
    "output_model_dir = './models/'\n",
    "#如果一开始用了并行训练最好加上这句\n",
    "model_to_save = best_model.module if hasattr(model, 'module') else model\n",
    "#这样保存的是模型参数，记得格式是.pt\n",
    "torch.save(model_to_save.state_dict(),output_model_dir+\"TRMmodel.pt\")\n",
    "\n",
    "\n",
    "# # 加载模型\n",
    "# #因为是自定义模型呀\n",
    "# model = Model()\n",
    "# #拿到保存的参数\n",
    "# model_static_dict = torch.load(output_model_dir+\"TRMmodel.pt\")\n",
    "# #把参数加载到模型中\n",
    "# model.load_state_dict(model_static_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d01bcf65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T03:04:27.754284Z",
     "start_time": "2023-03-07T03:04:27.241086Z"
    }
   },
   "outputs": [],
   "source": [
    "# 尝试保存模型\n",
    "torch.save(model, output_model_dir+\"TRM1model.pt\")\n",
    "# 加载模型\n",
    "model = torch.load(output_model_dir+\"TRM1model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "014de2ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T03:04:32.580942Z",
     "start_time": "2023-03-07T03:04:32.565936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransAm(\n",
       "  (pos_encoder): PositionalEncoding()\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "    (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60966a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "xai3",
   "language": "python",
   "name": "xai3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
